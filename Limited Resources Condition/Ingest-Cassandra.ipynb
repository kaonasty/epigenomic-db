{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6333775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cassandra-driver\n",
      "  Downloading cassandra_driver-3.29.2-cp311-cp311-win_amd64.whl.metadata (6.3 kB)\n",
      "Collecting geomet<0.3,>=0.1 (from cassandra-driver)\n",
      "  Using cached geomet-0.2.1.post1-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting click (from geomet<0.3,>=0.1->cassandra-driver)\n",
      "  Downloading click-8.2.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: six in c:\\users\\marco\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from geomet<0.3,>=0.1->cassandra-driver) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\marco\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from click->geomet<0.3,>=0.1->cassandra-driver) (0.4.6)\n",
      "Downloading cassandra_driver-3.29.2-cp311-cp311-win_amd64.whl (348 kB)\n",
      "Using cached geomet-0.2.1.post1-py3-none-any.whl (18 kB)\n",
      "Downloading click-8.2.0-py3-none-any.whl (102 kB)\n",
      "Installing collected packages: click, geomet, cassandra-driver\n",
      "\n",
      "   -------------------------- ------------- 2/3 [cassandra-driver]\n",
      "   -------------------------- ------------- 2/3 [cassandra-driver]\n",
      "   ---------------------------------------- 3/3 [cassandra-driver]\n",
      "\n",
      "Successfully installed cassandra-driver-3.29.2 click-8.2.0 geomet-0.2.1.post1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install cassandra-driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba7b6ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.23.2 (from pandas)\n",
      "  Downloading numpy-2.2.5-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\marco\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\marco\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.2.3-cp311-cp311-win_amd64.whl (11.6 MB)\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "   ------------------------------------- -- 10.7/11.6 MB 55.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.6/11.6 MB 42.7 MB/s eta 0:00:00\n",
      "Downloading numpy-2.2.5-cp311-cp311-win_amd64.whl (12.9 MB)\n",
      "   ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "   ---------------------------------- ----- 11.3/12.9 MB 58.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.9/12.9 MB 35.3 MB/s eta 0:00:00\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ---------------------------------------- 4/4 [pandas]\n",
      "\n",
      "Successfully installed numpy-2.2.5 pandas-2.2.3 pytz-2025.2 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38ef01d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     50\u001b[39m region_id = \u001b[38;5;28mstr\u001b[39m(uuid.uuid4())\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# Insert into region table\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\"\"\u001b[39;49m\n\u001b[32m     54\u001b[39m \u001b[33;43m    INSERT INTO region (id, chrom, start, end, strand, cell_line, region_type, window_size)\u001b[39;49m\n\u001b[32m     55\u001b[39m \u001b[33;43m    VALUES (\u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[33;43m, \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[33;43m, \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[33;43m, \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[33;43m, \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[33;43m, \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[33;43m, \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[33;43m, \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[33;43m)\u001b[39;49m\n\u001b[32m     56\u001b[39m \u001b[33;43m\u001b[39;49m\u001b[33;43m\"\"\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mregion_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mchrom\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mstart\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mend\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m      \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mstrand\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell_line\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregion_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# Prepare epigenomic features\u001b[39;00m\n\u001b[32m     60\u001b[39m feature_cols = [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df.columns \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mchrom\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mstart\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mend\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mstrand\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mTPM\u001b[39m\u001b[33m'\u001b[39m]]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\cassandra\\cluster.py:2677\u001b[39m, in \u001b[36mSession.execute\u001b[39m\u001b[34m(self, query, parameters, timeout, trace, custom_payload, execution_profile, paging_state, host, execute_as)\u001b[39m\n\u001b[32m   2634\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexecute\u001b[39m(\u001b[38;5;28mself\u001b[39m, query, parameters=\u001b[38;5;28;01mNone\u001b[39;00m, timeout=_NOT_SET, trace=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2635\u001b[39m             custom_payload=\u001b[38;5;28;01mNone\u001b[39;00m, execution_profile=EXEC_PROFILE_DEFAULT,\n\u001b[32m   2636\u001b[39m             paging_state=\u001b[38;5;28;01mNone\u001b[39;00m, host=\u001b[38;5;28;01mNone\u001b[39;00m, execute_as=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   2637\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2638\u001b[39m \u001b[33;03m    Execute the given query and synchronously wait for the response.\u001b[39;00m\n\u001b[32m   2639\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2674\u001b[39m \u001b[33;03m    on a DSE cluster.\u001b[39;00m\n\u001b[32m   2675\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2677\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_payload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_profile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaging_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecute_as\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\cassandra\\cluster.py:4952\u001b[39m, in \u001b[36mResponseFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   4927\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mresult\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   4928\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4929\u001b[39m \u001b[33;03m    Return the final result or raise an Exception if errors were\u001b[39;00m\n\u001b[32m   4930\u001b[39m \u001b[33;03m    encountered.  If the final result or error has not been set\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   4950\u001b[39m \n\u001b[32m   4951\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4952\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_event\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4953\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _NOT_SET:\n\u001b[32m   4954\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m ResultSet(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m._final_result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\threading.py:629\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    627\u001b[39m signaled = \u001b[38;5;28mself\u001b[39m._flag\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m629\u001b[39m     signaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\threading.py:327\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    329\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import uuid\n",
    "from cassandra.cluster import Cluster\n",
    "\n",
    "# Connect to Cassandra\n",
    "cluster = Cluster(['127.0.0.1'])\n",
    "session = cluster.connect()\n",
    "session.set_keyspace('bioinfo')\n",
    "\n",
    "# Create tables (optional if already created)\n",
    "session.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS region (\n",
    "    id TEXT PRIMARY KEY,\n",
    "    chrom TEXT,\n",
    "    start INT,\n",
    "    end INT,\n",
    "    strand TEXT,\n",
    "    cell_line TEXT,\n",
    "    region_type TEXT,\n",
    "    window_size INT\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "session.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS epigenomic_features (\n",
    "    id TEXT PRIMARY KEY,\n",
    "    region_id TEXT,\n",
    "    tpm FLOAT,\n",
    "    features MAP<TEXT, FLOAT>\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# Load CSVs\n",
    "folder = \"E:/DataTesis/Final_Epigenomic\"\n",
    "\n",
    "for filename in os.listdir(folder):\n",
    "    if not filename.endswith(\".csv\"):\n",
    "        continue\n",
    "\n",
    "    # Parse metadata from filename\n",
    "    parts = filename.replace(\".csv\", \"\").split(\"_\")\n",
    "    cell_line, window_size, region_type = parts[0], int(parts[1]), parts[2]\n",
    "\n",
    "    # Read CSV\n",
    "    df = pd.read_csv(os.path.join(folder, filename))\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        # Generate UUID for region \n",
    "        region_id = str(uuid.uuid4())\n",
    "\n",
    "        # Insert into region table\n",
    "        session.execute(\"\"\"\n",
    "            INSERT INTO region (id, chrom, start, end, strand, cell_line, region_type, window_size)\n",
    "            VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        \"\"\", (region_id, row['chrom'], int(row['start']), int(row['end']),\n",
    "              row['strand'], cell_line, region_type, window_size))\n",
    "\n",
    "        # Prepare epigenomic features\n",
    "        feature_cols = [col for col in df.columns if col not in ['chrom', 'start', 'end', 'strand', 'TPM']]\n",
    "        features = {col: float(row[col]) for col in feature_cols if pd.notnull(row[col])}\n",
    "\n",
    "        # Insert into epigenomic_features table\n",
    "        session.execute(\"\"\"\n",
    "            INSERT INTO epigenomic_features (id, region_id, tpm, features)\n",
    "            VALUES (%s, %s, %s, %s)\n",
    "        \"\"\", (str(uuid.uuid4()), region_id, float(row['TPM']), features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f7ed4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    cell_line region_type  window_size\n",
      "0          H1    promoter          512\n",
      "1          H1    promoter          256\n",
      "2        K562    promoter          512\n",
      "3        A549    enhancer         1024\n",
      "4        A549    enhancer          128\n",
      "..        ...         ...          ...\n",
      "203      K562    promoter          256\n",
      "207    HEK293    enhancer          128\n",
      "227     HepG2    enhancer          512\n",
      "258      K562    enhancer          128\n",
      "267    HEK293    enhancer         1024\n",
      "\n",
      "[62 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "import pandas as pd\n",
    "\n",
    "cluster = Cluster(['127.0.0.1'])\n",
    "session = cluster.connect('bioinfo')\n",
    "\n",
    "rows = session.execute(\"SELECT cell_line, region_type, window_size FROM region\")\n",
    "df = pd.DataFrame(rows, columns=['cell_line', 'region_type', 'window_size'])\n",
    "\n",
    "# Get distinct combinations\n",
    "distinct_combinations = df.drop_duplicates()\n",
    "print(distinct_combinations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d17c4751",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 9/9 [8:45:52<00:00, 3505.79s/it]  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import uuid\n",
    "from cassandra.cluster import Cluster\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Connect to Cassandra\n",
    "cluster = Cluster(['127.0.0.1'])\n",
    "session = cluster.connect()\n",
    "session.set_keyspace('bioinfo')\n",
    "\n",
    "# Create tables (optional)\n",
    "session.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS region (\n",
    "    id TEXT PRIMARY KEY,\n",
    "    chrom TEXT,\n",
    "    start INT,\n",
    "    end INT,\n",
    "    strand TEXT,\n",
    "    cell_line TEXT,\n",
    "    region_type TEXT,\n",
    "    window_size INT\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "session.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS epigenomic_features (\n",
    "    id TEXT PRIMARY KEY,\n",
    "    region_id TEXT,\n",
    "    tpm FLOAT,\n",
    "    features MAP<TEXT, FLOAT>\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# Load list of already processed files\n",
    "processed_files_path = \"processed_files.txt\"\n",
    "if os.path.exists(processed_files_path):\n",
    "    with open(processed_files_path, \"r\") as f:\n",
    "        processed_files = set(line.strip() for line in f)\n",
    "else:\n",
    "    processed_files = set()\n",
    "\n",
    "# Folder containing CSVs\n",
    "folder = \"E:/DataTesis/Final_Epigenomic\"\n",
    "all_files = [f for f in os.listdir(folder) if f.endswith(\".csv\") and f not in processed_files]\n",
    "\n",
    "# Loop through unprocessed files with progress bar\n",
    "for filename in tqdm(all_files, desc=\"Processing files\"):\n",
    "    parts = filename.replace(\".csv\", \"\").split(\"_\")\n",
    "    cell_line, window_size, region_type = parts[0], int(parts[1]), parts[2]\n",
    "\n",
    "    df = pd.read_csv(os.path.join(folder, filename))\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"Inserting rows from {filename}\", leave=False):\n",
    "        region_id = str(uuid.uuid4())\n",
    "\n",
    "        # Insert into region table\n",
    "        session.execute(\"\"\"\n",
    "            INSERT INTO region (id, chrom, start, end, strand, cell_line, region_type, window_size)\n",
    "            VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        \"\"\", (region_id, row['chrom'], int(row['start']), int(row['end']),\n",
    "              row['strand'], cell_line, region_type, window_size))\n",
    "\n",
    "        # Prepare feature map\n",
    "        feature_cols = [col for col in df.columns if col not in ['chrom', 'start', 'end', 'strand', 'TPM']]\n",
    "        features = {col: float(row[col]) for col in feature_cols if pd.notnull(row[col])}\n",
    "\n",
    "        session.execute(\"\"\"\n",
    "            INSERT INTO epigenomic_features (id, region_id, tpm, features)\n",
    "            VALUES (%s, %s, %s, %s)\n",
    "        \"\"\", (str(uuid.uuid4()), region_id, float(row['TPM']), features))\n",
    "\n",
    "    # Mark the file as processed\n",
    "    with open(processed_files_path, \"a\") as f:\n",
    "        f.write(filename + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9066cdf4",
   "metadata": {},
   "source": [
    "Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17cd778f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regions with window_size=256: 1142428\n",
      "Processed 10000 samples\n",
      "Processed 20000 samples\n",
      "Processed 30000 samples\n",
      "Processed 40000 samples\n",
      "Processed 50000 samples\n",
      "Processed 60000 samples\n",
      "Processed 70000 samples\n",
      "Processed 80000 samples\n",
      "Processed 90000 samples\n",
      "Processed 100000 samples\n",
      "Processed 110000 samples\n",
      "Processed 120000 samples\n",
      "Processed 130000 samples\n",
      "Processed 140000 samples\n",
      "Processed 150000 samples\n",
      "Processed 160000 samples\n",
      "Processed 170000 samples\n",
      "Processed 180000 samples\n",
      "Processed 190000 samples\n",
      "Processed 200000 samples\n",
      "Processed 210000 samples\n",
      "Processed 220000 samples\n",
      "Processed 230000 samples\n",
      "Processed 240000 samples\n",
      "Processed 250000 samples\n",
      "Processed 260000 samples\n",
      "Processed 270000 samples\n",
      "Processed 280000 samples\n",
      "Processed 290000 samples\n",
      "Processed 300000 samples\n",
      "Processed 310000 samples\n",
      "Processed 320000 samples\n",
      "Processed 330000 samples\n",
      "Processed 340000 samples\n",
      "Processed 350000 samples\n",
      "Processed 360000 samples\n",
      "Processed 370000 samples\n",
      "Processed 380000 samples\n",
      "Processed 390000 samples\n",
      "Processed 400000 samples\n",
      "Processed 410000 samples\n",
      "Processed 420000 samples\n",
      "Processed 430000 samples\n",
      "Processed 440000 samples\n",
      "Processed 450000 samples\n",
      "Processed 460000 samples\n",
      "Processed 470000 samples\n",
      "Processed 480000 samples\n",
      "Processed 490000 samples\n",
      "Processed 500000 samples\n",
      "Processed 510000 samples\n",
      "Processed 520000 samples\n",
      "Processed 530000 samples\n",
      "Processed 540000 samples\n",
      "Processed 550000 samples\n",
      "Processed 560000 samples\n",
      "Processed 570000 samples\n",
      "Processed 580000 samples\n",
      "Processed 590000 samples\n",
      "Processed 600000 samples\n",
      "Processed 610000 samples\n",
      "Processed 620000 samples\n",
      "Processed 630000 samples\n",
      "Processed 640000 samples\n",
      "Processed 650000 samples\n",
      "Processed 660000 samples\n",
      "Processed 670000 samples\n",
      "Processed 680000 samples\n",
      "Processed 690000 samples\n",
      "Processed 700000 samples\n",
      "Total samples collected: 709004\n",
      "Total unique features: 58\n",
      "Data extraction time: 492.70 seconds\n",
      "Epoch 1/10, Loss: 0.5318\n",
      "Epoch 2/10, Loss: 0.5228\n",
      "Epoch 3/10, Loss: 0.5204\n",
      "Epoch 4/10, Loss: 0.5190\n",
      "Epoch 5/10, Loss: 0.5177\n",
      "Epoch 6/10, Loss: 0.5168\n",
      "Epoch 7/10, Loss: 0.5162\n",
      "Epoch 8/10, Loss: 0.5157\n",
      "Epoch 9/10, Loss: 0.5150\n",
      "Epoch 10/10, Loss: 0.5147\n",
      "Test Accuracy: 0.7564\n",
      "Training time: 185.43 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from cassandra.cluster import Cluster\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# --- Connect to Cassandra ---\n",
    "cluster = Cluster(['127.0.0.1'])\n",
    "session = cluster.connect()\n",
    "session.set_keyspace('bioinfo')\n",
    "\n",
    "# --- Step 1: Extract data with time measurement ---\n",
    "start_extract = time.time()\n",
    "\n",
    "# Query to get region ids for window_size=256\n",
    "region_query = \"SELECT id, region_type FROM region WHERE window_size=256 ALLOW FILTERING\"\n",
    "regions = session.execute(region_query)\n",
    "\n",
    "region_dict = {row.id: row.region_type for row in regions}\n",
    "print(f\"Regions with window_size=256: {len(region_dict)}\")\n",
    "\n",
    "features_list = []\n",
    "labels_list = []\n",
    "\n",
    "# Query epigenomic_features (no filtering in query; filter in code)\n",
    "query = \"SELECT features, region_id, tpm FROM epigenomic_features\"\n",
    "future = session.execute_async(query)\n",
    "result = future.result()\n",
    "\n",
    "count = 0\n",
    "for row in result:\n",
    "    if row.region_id not in region_dict:\n",
    "        continue\n",
    "\n",
    "    region_type = region_dict[row.region_id]\n",
    "    if region_type not in ('promoter', 'enhancer'):\n",
    "        continue\n",
    "\n",
    "    # Only consider inactive regions (TPM==0) for binary classification\n",
    "    if row.tpm == 0.0:\n",
    "        label = 0 if region_type == 'enhancer' else 1  # enhancer=0, promoter=1\n",
    "\n",
    "        # Append features and label together here\n",
    "        feat_dict = dict(row.features)\n",
    "        features_list.append(feat_dict)\n",
    "        labels_list.append(label)\n",
    "\n",
    "        count += 1\n",
    "        if count % 10000 == 0:\n",
    "            print(f\"Processed {count} samples\")\n",
    "    else:\n",
    "        # Skip active regions (TPM!=0)\n",
    "        continue\n",
    "\n",
    "print(f\"Total samples collected: {len(labels_list)}\")\n",
    "\n",
    "# Get union of all feature keys to create fixed feature vector columns\n",
    "all_features = set()\n",
    "for d in features_list:\n",
    "    all_features.update(d.keys())\n",
    "all_features = sorted(all_features)\n",
    "\n",
    "print(f\"Total unique features: {len(all_features)}\")\n",
    "\n",
    "# Build numpy array X of shape (samples, features)\n",
    "X = np.zeros((len(labels_list), len(all_features)), dtype=np.float32)\n",
    "for i, feat_dict in enumerate(features_list):\n",
    "    for j, key in enumerate(all_features):\n",
    "        X[i, j] = feat_dict.get(key, 0.0)\n",
    "\n",
    "y = np.array(labels_list, dtype=np.int64)\n",
    "\n",
    "end_extract = time.time()\n",
    "print(f\"Data extraction time: {end_extract - start_extract:.2f} seconds\")\n",
    "\n",
    "# --- Step 2: Prepare data for training ---\n",
    "start_train = time.time()\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_train_t = torch.tensor(X_train)\n",
    "y_train_t = torch.tensor(y_train)\n",
    "X_test_t = torch.tensor(X_test)\n",
    "y_test_t = torch.tensor(y_test)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "# --- Define FFNN model ---\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(FFNN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = FFNN(input_dim=X.shape[1])\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for xb, yb in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb).squeeze()\n",
    "        loss = criterion(preds, yb.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dl.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = model(X_test_t).squeeze()\n",
    "    preds_label = (preds > 0.5).long()\n",
    "    accuracy = (preds_label == y_test_t).float().mean().item()\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "end_train = time.time()\n",
    "print(f\"Training time: {end_train - start_train:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f831b6",
   "metadata": {},
   "source": [
    "Model FIX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8305932c",
   "metadata": {},
   "source": [
    "IE vs IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a175c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regions with window_size=256: 1142428\n",
      "Processed 10000 samples\n",
      "Processed 20000 samples\n",
      "Processed 30000 samples\n",
      "Processed 40000 samples\n",
      "Processed 50000 samples\n",
      "Processed 60000 samples\n",
      "Processed 70000 samples\n",
      "Processed 80000 samples\n",
      "Processed 90000 samples\n",
      "Processed 100000 samples\n",
      "Processed 110000 samples\n",
      "Processed 120000 samples\n",
      "Processed 130000 samples\n",
      "Processed 140000 samples\n",
      "Processed 150000 samples\n",
      "Processed 160000 samples\n",
      "Processed 170000 samples\n",
      "Processed 180000 samples\n",
      "Processed 190000 samples\n",
      "Processed 200000 samples\n",
      "Processed 210000 samples\n",
      "Processed 220000 samples\n",
      "Processed 230000 samples\n",
      "Processed 240000 samples\n",
      "Processed 250000 samples\n",
      "Processed 260000 samples\n",
      "Processed 270000 samples\n",
      "Processed 280000 samples\n",
      "Processed 290000 samples\n",
      "Processed 300000 samples\n",
      "Processed 310000 samples\n",
      "Processed 320000 samples\n",
      "Processed 330000 samples\n",
      "Processed 340000 samples\n",
      "Processed 350000 samples\n",
      "Processed 360000 samples\n",
      "Processed 370000 samples\n",
      "Processed 380000 samples\n",
      "Processed 390000 samples\n",
      "Processed 400000 samples\n",
      "Processed 410000 samples\n",
      "Processed 420000 samples\n",
      "Processed 430000 samples\n",
      "Processed 440000 samples\n",
      "Processed 450000 samples\n",
      "Processed 460000 samples\n",
      "Processed 470000 samples\n",
      "Processed 480000 samples\n",
      "Processed 490000 samples\n",
      "Processed 500000 samples\n",
      "Processed 510000 samples\n",
      "Processed 520000 samples\n",
      "Processed 530000 samples\n",
      "Processed 540000 samples\n",
      "Processed 550000 samples\n",
      "Processed 560000 samples\n",
      "Processed 570000 samples\n",
      "Processed 580000 samples\n",
      "Processed 590000 samples\n",
      "Processed 600000 samples\n",
      "Processed 610000 samples\n",
      "Processed 620000 samples\n",
      "Processed 630000 samples\n",
      "Processed 640000 samples\n",
      "Processed 650000 samples\n",
      "Processed 660000 samples\n",
      "Processed 670000 samples\n",
      "Processed 680000 samples\n",
      "Processed 690000 samples\n",
      "Processed 700000 samples\n",
      "Total samples collected: 709004\n",
      "Total unique features: 58\n",
      "Data extraction time: 704.56 seconds\n",
      "Epoch 1/64, Loss: 0.5461, LR: 0.05000\n",
      "Epoch 2/64, Loss: 0.5273, LR: 0.00500\n",
      "Epoch 3/64, Loss: 0.5232, LR: 0.00050\n",
      "Epoch 4/64, Loss: 0.5227, LR: 0.00005\n",
      "Epoch 5/64, Loss: 0.5226, LR: 0.00001\n",
      "Epoch 6/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 7/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 8/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 9/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 10/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 11/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 12/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 13/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 14/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 15/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 16/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 17/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 18/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 19/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 20/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 21/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 22/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 23/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 24/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 25/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 26/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 27/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 28/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 29/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 30/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 31/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 32/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 33/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 34/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 35/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 36/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 37/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 38/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 39/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 40/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 41/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 42/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 43/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 44/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 45/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 46/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 47/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 48/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 49/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 50/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 51/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 52/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 53/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 54/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 55/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 56/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 57/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 58/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 59/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 60/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 61/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 62/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 63/64, Loss: 0.5226, LR: 0.00000\n",
      "Epoch 64/64, Loss: 0.5226, LR: 0.00000\n",
      "Test Accuracy: 0.7516\n",
      "Training time: 1328.72 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from cassandra.cluster import Cluster\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# --- Connect to Cassandra ---\n",
    "cluster = Cluster(['127.0.0.1'])\n",
    "session = cluster.connect()\n",
    "session.set_keyspace('bioinfo')\n",
    "\n",
    "# --- Step 1: Extract data with time measurement ---\n",
    "start_extract = time.time()\n",
    "\n",
    "# Query to get region ids for window_size=256\n",
    "region_query = \"SELECT id, region_type FROM region WHERE window_size=256 ALLOW FILTERING\"\n",
    "regions = session.execute(region_query)\n",
    "\n",
    "region_dict = {row.id: row.region_type for row in regions}\n",
    "print(f\"Regions with window_size=256: {len(region_dict)}\")\n",
    "\n",
    "features_list = []\n",
    "labels_list = []\n",
    "\n",
    "# Query epigenomic_features (no filtering in query; filter in code)\n",
    "query = \"SELECT features, region_id, tpm FROM epigenomic_features\"\n",
    "future = session.execute_async(query)\n",
    "result = future.result()\n",
    "\n",
    "count = 0\n",
    "for row in result:\n",
    "    if row.region_id not in region_dict:\n",
    "        continue\n",
    "\n",
    "    region_type = region_dict[row.region_id]\n",
    "    if region_type not in ('promoter', 'enhancer'):\n",
    "        continue\n",
    "\n",
    "    # Only consider inactive regions (TPM==0) for binary classification\n",
    "    if row.tpm == 0.0:\n",
    "        label = 0 if region_type == 'enhancer' else 1  # enhancer=0, promoter=1\n",
    "\n",
    "        # Append features and label together here\n",
    "        feat_dict = dict(row.features)\n",
    "        features_list.append(feat_dict)\n",
    "        labels_list.append(label)\n",
    "\n",
    "        count += 1\n",
    "        if count % 10000 == 0:\n",
    "            print(f\"Processed {count} samples\")\n",
    "    else:\n",
    "        # Skip active regions (TPM!=0)\n",
    "        continue\n",
    "\n",
    "print(f\"Total samples collected: {len(labels_list)}\")\n",
    "\n",
    "# Get union of all feature keys to create fixed feature vector columns\n",
    "all_features = set()\n",
    "for d in features_list:\n",
    "    all_features.update(d.keys())\n",
    "all_features = sorted(all_features)\n",
    "\n",
    "print(f\"Total unique features: {len(all_features)}\")\n",
    "\n",
    "# Build numpy array X of shape (samples, features)\n",
    "X = np.zeros((len(labels_list), len(all_features)), dtype=np.float32)\n",
    "for i, feat_dict in enumerate(features_list):\n",
    "    for j, key in enumerate(all_features):\n",
    "        X[i, j] = feat_dict.get(key, 0.0)\n",
    "\n",
    "y = np.array(labels_list, dtype=np.int64)\n",
    "\n",
    "end_extract = time.time()\n",
    "print(f\"Data extraction time: {end_extract - start_extract:.2f} seconds\")\n",
    "\n",
    "# --- Step 2: Prepare data for training ---\n",
    "start_train = time.time()\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_train_t = torch.tensor(X_train)\n",
    "y_train_t = torch.tensor(y_train)\n",
    "X_test_t = torch.tensor(X_test)\n",
    "y_test_t = torch.tensor(y_test)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "# --- Define FFNN model with updated config ---\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(FFNN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = FFNN(input_dim=X.shape[1])\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.5, weight_decay=0.0)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)  # decay learning rate\n",
    "\n",
    "# Use batch size 32\n",
    "train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "epochs = 64\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for xb, yb in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb).squeeze()\n",
    "        loss = criterion(preds, yb.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    scheduler.step()  # Apply learning rate decay\n",
    "    epoch_loss = running_loss / len(train_dl.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.5f}\")\n",
    "\n",
    "\n",
    "# Evaluate on test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = model(X_test_t).squeeze()\n",
    "    preds_label = (preds > 0.5).long()\n",
    "    accuracy = (preds_label == y_test_t).float().mean().item()\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "end_train = time.time()\n",
    "print(f\"Training time: {end_train - start_train:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26425da0",
   "metadata": {},
   "source": [
    "AP vs IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abad511a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regions with window_size=256: 1142428\n",
      "Processed 10000 samples\n",
      "Processed 20000 samples\n",
      "Processed 30000 samples\n",
      "Processed 40000 samples\n",
      "Processed 50000 samples\n",
      "Processed 60000 samples\n",
      "Processed 70000 samples\n",
      "Processed 80000 samples\n",
      "Processed 90000 samples\n",
      "Processed 100000 samples\n",
      "Processed 110000 samples\n",
      "Processed 120000 samples\n",
      "Processed 130000 samples\n",
      "Processed 140000 samples\n",
      "Processed 150000 samples\n",
      "Processed 160000 samples\n",
      "Processed 170000 samples\n",
      "Processed 180000 samples\n",
      "Processed 190000 samples\n",
      "Processed 200000 samples\n",
      "Processed 210000 samples\n",
      "Processed 220000 samples\n",
      "Processed 230000 samples\n",
      "Processed 240000 samples\n",
      "Processed 250000 samples\n",
      "Processed 260000 samples\n",
      "Processed 270000 samples\n",
      "Processed 280000 samples\n",
      "Processed 290000 samples\n",
      "Processed 300000 samples\n",
      "Processed 310000 samples\n",
      "Processed 320000 samples\n",
      "Processed 330000 samples\n",
      "Processed 340000 samples\n",
      "Processed 350000 samples\n",
      "Processed 360000 samples\n",
      "Processed 370000 samples\n",
      "Processed 380000 samples\n",
      "Processed 390000 samples\n",
      "Processed 400000 samples\n",
      "Processed 410000 samples\n",
      "Processed 420000 samples\n",
      "Processed 430000 samples\n",
      "Processed 440000 samples\n",
      "Processed 450000 samples\n",
      "Processed 460000 samples\n",
      "Processed 470000 samples\n",
      "Processed 480000 samples\n",
      "Processed 490000 samples\n",
      "Processed 500000 samples\n",
      "Processed 510000 samples\n",
      "Processed 520000 samples\n",
      "Processed 530000 samples\n",
      "Processed 540000 samples\n",
      "Processed 550000 samples\n",
      "Processed 560000 samples\n",
      "Processed 570000 samples\n",
      "Processed 580000 samples\n",
      "Processed 590000 samples\n",
      "Processed 600000 samples\n",
      "Processed 610000 samples\n",
      "Processed 620000 samples\n",
      "Processed 630000 samples\n",
      "Processed 640000 samples\n",
      "Processed 650000 samples\n",
      "Processed 660000 samples\n",
      "Processed 670000 samples\n",
      "Processed 680000 samples\n",
      "Processed 690000 samples\n",
      "Total samples collected: 699433\n",
      "Total unique features: 58\n",
      "Data extraction time: 464.74 seconds\n",
      "Epoch 1/64, Loss: 0.6786, LR: 0.05000\n",
      "Epoch 2/64, Loss: 0.6736, LR: 0.00500\n",
      "Epoch 3/64, Loss: 0.6727, LR: 0.00050\n",
      "Epoch 4/64, Loss: 0.6726, LR: 0.00005\n",
      "Epoch 5/64, Loss: 0.6726, LR: 0.00001\n",
      "Epoch 6/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 7/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 8/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 9/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 10/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 11/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 12/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 13/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 14/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 15/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 16/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 17/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 18/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 19/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 20/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 21/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 22/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 23/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 24/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 25/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 26/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 27/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 28/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 29/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 30/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 31/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 32/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 33/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 34/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 35/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 36/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 37/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 38/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 39/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 40/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 41/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 42/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 43/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 44/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 45/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 46/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 47/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 48/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 49/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 50/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 51/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 52/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 53/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 54/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 55/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 56/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 57/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 58/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 59/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 60/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 61/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 62/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 63/64, Loss: 0.6726, LR: 0.00000\n",
      "Epoch 64/64, Loss: 0.6726, LR: 0.00000\n",
      "Test Accuracy: 0.5663\n",
      "Training time: 1350.63 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from cassandra.cluster import Cluster\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# --- Connect to Cassandra ---\n",
    "cluster = Cluster(['127.0.0.1'])\n",
    "session = cluster.connect()\n",
    "session.set_keyspace('bioinfo')\n",
    "\n",
    "# --- Step 1: Extract data with time measurement ---\n",
    "start_extract = time.time()\n",
    "\n",
    "# Query to get region ids for window_size=256\n",
    "region_query = \"SELECT id, region_type FROM region WHERE window_size=256 ALLOW FILTERING\"\n",
    "regions = session.execute(region_query)\n",
    "\n",
    "region_dict = {row.id: row.region_type for row in regions}\n",
    "print(f\"Regions with window_size=256: {len(region_dict)}\")\n",
    "\n",
    "features_list = []\n",
    "labels_list = []\n",
    "\n",
    "# Query epigenomic_features (no filtering in query; filter in code)\n",
    "query = \"SELECT features, region_id, tpm FROM epigenomic_features\"\n",
    "future = session.execute_async(query)\n",
    "result = future.result()\n",
    "\n",
    "count = 0\n",
    "for row in result:\n",
    "    if row.region_id not in region_dict:\n",
    "        continue\n",
    "\n",
    "    region_type = region_dict[row.region_id]\n",
    "    if region_type !='promoter':\n",
    "        continue\n",
    "\n",
    "    # Label inactive (TPM == 0) as 0, active (TPM > 0) as 1\n",
    "    label = 0 if row.tpm == 0.0 else 1\n",
    "\n",
    "    # Append features and label together here\n",
    "    feat_dict = dict(row.features)\n",
    "    features_list.append(feat_dict)\n",
    "    labels_list.append(label)\n",
    "\n",
    "    count += 1\n",
    "    if count % 10000 == 0:\n",
    "        print(f\"Processed {count} samples\")\n",
    "\n",
    "print(f\"Total samples collected: {len(labels_list)}\")\n",
    "\n",
    "# Get union of all feature keys to create fixed feature vector columns\n",
    "all_features = set()\n",
    "for d in features_list:\n",
    "    all_features.update(d.keys())\n",
    "all_features = sorted(all_features)\n",
    "\n",
    "print(f\"Total unique features: {len(all_features)}\")\n",
    "\n",
    "# Build numpy array X of shape (samples, features)\n",
    "X = np.zeros((len(labels_list), len(all_features)), dtype=np.float32)\n",
    "for i, feat_dict in enumerate(features_list):\n",
    "    for j, key in enumerate(all_features):\n",
    "        X[i, j] = feat_dict.get(key, 0.0)\n",
    "\n",
    "y = np.array(labels_list, dtype=np.int64)\n",
    "\n",
    "end_extract = time.time()\n",
    "print(f\"Data extraction time: {end_extract - start_extract:.2f} seconds\")\n",
    "\n",
    "# --- Step 2: Prepare data for training ---\n",
    "start_train = time.time()\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_train_t = torch.tensor(X_train)\n",
    "y_train_t = torch.tensor(y_train)\n",
    "X_test_t = torch.tensor(X_test)\n",
    "y_test_t = torch.tensor(y_test)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "# --- Define FFNN model with updated config ---\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(FFNN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = FFNN(input_dim=X.shape[1])\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.5, weight_decay=0.0)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)  # decay learning rate\n",
    "\n",
    "# Use batch size 32\n",
    "train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "epochs = 64\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for xb, yb in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb).squeeze()\n",
    "        loss = criterion(preds, yb.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    scheduler.step()  # Apply learning rate decay\n",
    "    epoch_loss = running_loss / len(train_dl.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.5f}\")\n",
    "\n",
    "\n",
    "# Evaluate on test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = model(X_test_t).squeeze()\n",
    "    preds_label = (preds > 0.5).long()\n",
    "    accuracy = (preds_label == y_test_t).float().mean().item()\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "end_train = time.time()\n",
    "print(f\"Training time: {end_train - start_train:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1be7c24",
   "metadata": {},
   "source": [
    "AE vs IE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b66d683a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regions with window_size=256: 1142428\n",
      "Processed 10000 samples\n",
      "Processed 20000 samples\n",
      "Processed 30000 samples\n",
      "Processed 40000 samples\n",
      "Processed 50000 samples\n",
      "Processed 60000 samples\n",
      "Processed 70000 samples\n",
      "Processed 80000 samples\n",
      "Processed 90000 samples\n",
      "Processed 100000 samples\n",
      "Processed 110000 samples\n",
      "Processed 120000 samples\n",
      "Processed 130000 samples\n",
      "Processed 140000 samples\n",
      "Processed 150000 samples\n",
      "Processed 160000 samples\n",
      "Processed 170000 samples\n",
      "Processed 180000 samples\n",
      "Processed 190000 samples\n",
      "Processed 200000 samples\n",
      "Processed 210000 samples\n",
      "Processed 220000 samples\n",
      "Processed 230000 samples\n",
      "Processed 240000 samples\n",
      "Processed 250000 samples\n",
      "Processed 260000 samples\n",
      "Processed 270000 samples\n",
      "Processed 280000 samples\n",
      "Processed 290000 samples\n",
      "Processed 300000 samples\n",
      "Processed 310000 samples\n",
      "Processed 320000 samples\n",
      "Processed 330000 samples\n",
      "Processed 340000 samples\n",
      "Processed 350000 samples\n",
      "Processed 360000 samples\n",
      "Processed 370000 samples\n",
      "Processed 380000 samples\n",
      "Processed 390000 samples\n",
      "Processed 400000 samples\n",
      "Processed 410000 samples\n",
      "Processed 420000 samples\n",
      "Processed 430000 samples\n",
      "Processed 440000 samples\n",
      "Total samples collected: 442995\n",
      "Total unique features: 58\n",
      "Data extraction time: 496.65 seconds\n",
      "Epoch 1/64, Loss: 0.2617, LR: 0.05000\n",
      "Epoch 2/64, Loss: 0.2508, LR: 0.00500\n",
      "Epoch 3/64, Loss: 0.2490, LR: 0.00050\n",
      "Epoch 4/64, Loss: 0.2488, LR: 0.00005\n",
      "Epoch 5/64, Loss: 0.2487, LR: 0.00001\n",
      "Epoch 6/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 7/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 8/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 9/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 10/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 11/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 12/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 13/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 14/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 15/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 16/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 17/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 18/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 19/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 20/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 21/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 22/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 23/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 24/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 25/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 26/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 27/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 28/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 29/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 30/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 31/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 32/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 33/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 34/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 35/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 36/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 37/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 38/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 39/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 40/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 41/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 42/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 43/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 44/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 45/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 46/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 47/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 48/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 49/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 50/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 51/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 52/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 53/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 54/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 55/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 56/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 57/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 58/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 59/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 60/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 61/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 62/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 63/64, Loss: 0.2487, LR: 0.00000\n",
      "Epoch 64/64, Loss: 0.2487, LR: 0.00000\n",
      "Test Accuracy: 0.9174\n",
      "Training time: 811.73 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from cassandra.cluster import Cluster\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# --- Connect to Cassandra ---\n",
    "cluster = Cluster(['127.0.0.1'])\n",
    "session = cluster.connect()\n",
    "session.set_keyspace('bioinfo')\n",
    "\n",
    "# --- Step 1: Extract data with time measurement ---\n",
    "start_extract = time.time()\n",
    "\n",
    "# Query to get region ids for window_size=256\n",
    "region_query = \"SELECT id, region_type FROM region WHERE window_size=256 ALLOW FILTERING\"\n",
    "regions = session.execute(region_query)\n",
    "\n",
    "region_dict = {row.id: row.region_type for row in regions}\n",
    "print(f\"Regions with window_size=256: {len(region_dict)}\")\n",
    "\n",
    "features_list = []\n",
    "labels_list = []\n",
    "\n",
    "# Query epigenomic_features (no filtering in query; filter in code)\n",
    "query = \"SELECT features, region_id, tpm FROM epigenomic_features\"\n",
    "future = session.execute_async(query)\n",
    "result = future.result()\n",
    "\n",
    "count = 0\n",
    "for row in result:\n",
    "    if row.region_id not in region_dict:\n",
    "        continue\n",
    "\n",
    "    region_type = region_dict[row.region_id]\n",
    "    if region_type != 'enhancer':\n",
    "        continue\n",
    "\n",
    "    # Label inactive (TPM == 0) as 0, active (TPM > 0) as 1\n",
    "    label = 0 if row.tpm == 0.0 else 1\n",
    "\n",
    "    feat_dict = dict(row.features)\n",
    "    features_list.append(feat_dict)\n",
    "    labels_list.append(label)\n",
    "\n",
    "    count += 1\n",
    "    if count % 10000 == 0:\n",
    "        print(f\"Processed {count} samples\")\n",
    "\n",
    "print(f\"Total samples collected: {len(labels_list)}\")\n",
    "\n",
    "# Get union of all feature keys to create fixed feature vector columns\n",
    "all_features = set()\n",
    "for d in features_list:\n",
    "    all_features.update(d.keys())\n",
    "all_features = sorted(all_features)\n",
    "\n",
    "print(f\"Total unique features: {len(all_features)}\")\n",
    "\n",
    "# Build numpy array X of shape (samples, features)\n",
    "X = np.zeros((len(labels_list), len(all_features)), dtype=np.float32)\n",
    "for i, feat_dict in enumerate(features_list):\n",
    "    for j, key in enumerate(all_features):\n",
    "        X[i, j] = feat_dict.get(key, 0.0)\n",
    "\n",
    "y = np.array(labels_list, dtype=np.int64)\n",
    "\n",
    "end_extract = time.time()\n",
    "print(f\"Data extraction time: {end_extract - start_extract:.2f} seconds\")\n",
    "\n",
    "# --- Step 2: Prepare data for training ---\n",
    "start_train = time.time()\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_train_t = torch.tensor(X_train)\n",
    "y_train_t = torch.tensor(y_train)\n",
    "X_test_t = torch.tensor(X_test)\n",
    "y_test_t = torch.tensor(y_test)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "# --- Define FFNN model with updated config ---\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(FFNN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = FFNN(input_dim=X.shape[1])\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.5, weight_decay=0.0)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)  # decay learning rate\n",
    "\n",
    "# Use batch size 32\n",
    "train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "epochs = 64\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for xb, yb in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb).squeeze()\n",
    "        loss = criterion(preds, yb.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    scheduler.step()  # Apply learning rate decay\n",
    "    epoch_loss = running_loss / len(train_dl.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.5f}\")\n",
    "\n",
    "\n",
    "# Evaluate on test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = model(X_test_t).squeeze()\n",
    "    preds_label = (preds > 0.5).long()\n",
    "    accuracy = (preds_label == y_test_t).float().mean().item()\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "end_train = time.time()\n",
    "print(f\"Training time: {end_train - start_train:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a3fc1f",
   "metadata": {},
   "source": [
    "AE vs AP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8eb44da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regions with window_size=256: 1142428\n",
      "Processed 10000 samples\n",
      "Processed 20000 samples\n",
      "Processed 30000 samples\n",
      "Processed 40000 samples\n",
      "Processed 50000 samples\n",
      "Processed 60000 samples\n",
      "Processed 70000 samples\n",
      "Processed 80000 samples\n",
      "Total samples collected: 89815\n",
      "Total unique features: 58\n",
      "Data extraction time: 474.99 seconds\n",
      "Epoch 1/64, Loss: 0.3499, LR: 0.05000\n",
      "Epoch 2/64, Loss: 0.2919, LR: 0.00500\n",
      "Epoch 3/64, Loss: 0.2850, LR: 0.00050\n",
      "Epoch 4/64, Loss: 0.2839, LR: 0.00005\n",
      "Epoch 5/64, Loss: 0.2838, LR: 0.00001\n",
      "Epoch 6/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 7/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 8/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 9/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 10/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 11/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 12/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 13/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 14/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 15/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 16/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 17/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 18/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 19/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 20/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 21/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 22/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 23/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 24/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 25/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 26/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 27/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 28/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 29/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 30/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 31/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 32/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 33/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 34/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 35/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 36/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 37/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 38/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 39/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 40/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 41/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 42/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 43/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 44/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 45/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 46/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 47/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 48/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 49/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 50/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 51/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 52/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 53/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 54/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 55/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 56/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 57/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 58/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 59/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 60/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 61/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 62/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 63/64, Loss: 0.2838, LR: 0.00000\n",
      "Epoch 64/64, Loss: 0.2838, LR: 0.00000\n",
      "Test Accuracy: 0.8918\n",
      "Training time: 160.06 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from cassandra.cluster import Cluster\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# --- Connect to Cassandra ---\n",
    "cluster = Cluster(['127.0.0.1'])\n",
    "session = cluster.connect()\n",
    "session.set_keyspace('bioinfo')\n",
    "\n",
    "# --- Step 1: Extract data with time measurement ---\n",
    "start_extract = time.time()\n",
    "\n",
    "# Query to get region ids for window_size=256\n",
    "region_query = \"SELECT id, region_type FROM region WHERE window_size=256 ALLOW FILTERING\"\n",
    "regions = session.execute(region_query)\n",
    "\n",
    "region_dict = {row.id: row.region_type for row in regions}\n",
    "print(f\"Regions with window_size=256: {len(region_dict)}\")\n",
    "\n",
    "features_list = []\n",
    "labels_list = []\n",
    "\n",
    "# Query epigenomic_features (no filtering in query; filter in code)\n",
    "query = \"SELECT features, region_id, tpm FROM epigenomic_features\"\n",
    "future = session.execute_async(query)\n",
    "result = future.result()\n",
    "\n",
    "count = 0\n",
    "for row in result:\n",
    "    if row.region_id not in region_dict:\n",
    "        continue\n",
    "\n",
    "    region_type = region_dict[row.region_id]\n",
    "    if region_type not in ('enhancer', 'promoter'):\n",
    "        continue\n",
    "\n",
    "    # Only consider active regions (TPM > 0)\n",
    "    if row.tpm > 0.0:\n",
    "        label = 0 if region_type == 'enhancer' else 1  # enhancer=0, promoter=1\n",
    "\n",
    "        feat_dict = dict(row.features)\n",
    "        features_list.append(feat_dict)\n",
    "        labels_list.append(label)\n",
    "\n",
    "        count += 1\n",
    "        if count % 10000 == 0:\n",
    "            print(f\"Processed {count} samples\")\n",
    "    else:\n",
    "        continue  # Skip inactive\n",
    "\n",
    "print(f\"Total samples collected: {len(labels_list)}\")\n",
    "\n",
    "# Get union of all feature keys to create fixed feature vector columns\n",
    "all_features = set()\n",
    "for d in features_list:\n",
    "    all_features.update(d.keys())\n",
    "all_features = sorted(all_features)\n",
    "\n",
    "print(f\"Total unique features: {len(all_features)}\")\n",
    "\n",
    "# Build numpy array X of shape (samples, features)\n",
    "X = np.zeros((len(labels_list), len(all_features)), dtype=np.float32)\n",
    "for i, feat_dict in enumerate(features_list):\n",
    "    for j, key in enumerate(all_features):\n",
    "        X[i, j] = feat_dict.get(key, 0.0)\n",
    "\n",
    "y = np.array(labels_list, dtype=np.int64)\n",
    "\n",
    "end_extract = time.time()\n",
    "print(f\"Data extraction time: {end_extract - start_extract:.2f} seconds\")\n",
    "\n",
    "# --- Step 2: Prepare data for training ---\n",
    "start_train = time.time()\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_train_t = torch.tensor(X_train)\n",
    "y_train_t = torch.tensor(y_train)\n",
    "X_test_t = torch.tensor(X_test)\n",
    "y_test_t = torch.tensor(y_test)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "# --- Define FFNN model with updated config ---\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(FFNN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = FFNN(input_dim=X.shape[1])\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.5, weight_decay=0.0)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)  # decay learning rate\n",
    "\n",
    "# Use batch size 32\n",
    "train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "epochs = 64\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for xb, yb in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb).squeeze()\n",
    "        loss = criterion(preds, yb.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    scheduler.step()  # Apply learning rate decay\n",
    "    epoch_loss = running_loss / len(train_dl.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.5f}\")\n",
    "\n",
    "\n",
    "# Evaluate on test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = model(X_test_t).squeeze()\n",
    "    preds_label = (preds > 0.5).long()\n",
    "    accuracy = (preds_label == y_test_t).float().mean().item()\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "end_train = time.time()\n",
    "print(f\"Training time: {end_train - start_train:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac066be9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
