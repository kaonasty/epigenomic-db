{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93bcf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "from tqdm import tqdm\n",
    "\n",
    "# MySQL connection config\n",
    "db = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"B@ndung40175\",\n",
    "    database=\"epigenomic\"\n",
    ")\n",
    "cursor = db.cursor()\n",
    "\n",
    "# Folder with CSV files\n",
    "csv_folder = \"E:/DataTesis/Final_Epigenomic\"  # update if needed\n",
    "\n",
    "# Track processed files\n",
    "processed_files_path = \"processed_files.txt\"\n",
    "if os.path.exists(processed_files_path):\n",
    "    with open(processed_files_path, \"r\") as f:\n",
    "        already_processed = set(f.read().splitlines())\n",
    "else:\n",
    "    already_processed = set()\n",
    "\n",
    "# Get only unprocessed CSVs\n",
    "csv_files = [\n",
    "    f for f in os.listdir(csv_folder)\n",
    "    if f.endswith(\".csv\") and f not in already_processed\n",
    "]\n",
    "\n",
    "for file in tqdm(csv_files, desc=\"Processing CSV files\"):\n",
    "    file_path = os.path.join(csv_folder, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"Inserting rows from {file}\", leave=False):\n",
    "\n",
    "        # EXAMPLE: Replace this block with your actual logic based on the ER diagram\n",
    "        chrom = row['chrom']\n",
    "        start = int(row['start'])\n",
    "        end = int(row['end'])\n",
    "        strand = row['strand']\n",
    "        tpm = float(row['TPM'])\n",
    "        feature_name = row['feature_name']\n",
    "        feature_class = row['feature_class']\n",
    "        fold_change = float(row['fold_change'])\n",
    "        window_size = int(row['window_size'])\n",
    "        region_type = row['region_type']\n",
    "        cell_line = row['cell_line']\n",
    "\n",
    "        # You need to handle INSERTS like:\n",
    "        # - INSERT INTO GenomicRegion\n",
    "        # - INSERT INTO EpigenomicFeature\n",
    "        # - INSERT INTO EpigenomicActivity\n",
    "        # - INSERT INTO CellLine, WindowSize, etc.\n",
    "        # Make sure to deduplicate if needed (check before inserting)\n",
    "\n",
    "        # Example: Insert CellLine\n",
    "        cursor.execute(\"SELECT cell_line_id FROM CellLine WHERE name = %s\", (cell_line,))\n",
    "        result = cursor.fetchone()\n",
    "        if result:\n",
    "            cell_line_id = result[0]\n",
    "        else:\n",
    "            cursor.execute(\"INSERT INTO CellLine (name) VALUES (%s)\", (cell_line,))\n",
    "            db.commit()\n",
    "            cell_line_id = cursor.lastrowid\n",
    "\n",
    "        # Repeat for RegionType, WindowSize, GenomicRegion, etc.\n",
    "        # Then build relationships in EpigenomicActivity and FeatureValue\n",
    "\n",
    "    # Add to processed list\n",
    "    with open(processed_files_path, \"a\") as f:\n",
    "        f.write(file + \"\\n\")\n",
    "\n",
    "    print(f\"✅ Finished {file}\")\n",
    "\n",
    "cursor.close()\n",
    "db.close()\n",
    "print(\"✅ All files processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3edb764",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  11%|█         | 1/9 [5:21:51<42:54:52, 19311.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: H1_1024_promoter_epigenomic.csv\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'E:/DataTesis/Final_Epigenomic\\\\HEK293_1024_enhancer_epigenomic.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 93\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m tqdm(csv_files, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing CSV files\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     92\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(csv_folder, file)\n\u001b[1;32m---> 93\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m     base_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(file)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_epigenomic.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     96\u001b[0m     cell_line, window_size, region_type \u001b[38;5;241m=\u001b[39m base_name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'E:/DataTesis/Final_Epigenomic\\\\HEK293_1024_enhancer_epigenomic.csv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "from tqdm import tqdm\n",
    "\n",
    "def sanitize_value(val):\n",
    "    if val is None:\n",
    "        return None\n",
    "    if isinstance(val, float) and math.isnan(val):\n",
    "        return None\n",
    "    return val\n",
    "\n",
    "# Feature class map\n",
    "feature_class_map = {\n",
    "    \"Histone Modification\": [\n",
    "        \"H3K27ac\", \"H3K27me3\", \"H3K36me3\", \"H3K4me1\", \"H3K4me2\", \"H3K4me3\",\n",
    "        \"H3K79me2\", \"H3K9ac\", \"H3K9me2\", \"H3K9me3\", \"H4K20me1\"\n",
    "    ],\n",
    "    \"DNA Methylation\": [\"DNMT1\", \"DNMT3B\"],\n",
    "    \"Chromatin Remodeling and Histone Modifiers\": [\n",
    "        \"EZH2\", \"EED\", \"SUZ12\", \"KDM1A\", \"KDM2A\", \"KDM3A\", \"KDM4B\", \"KDM5A\",\n",
    "        \"KDM5B\", \"KDM6A\", \"KAT2A\", \"KAT2B\", \"KAT7\", \"KAT8\", \"HDAC1\", \"HDAC2\",\n",
    "        \"HDAC3\", \"HDAC6\", \"HDAC8\", \"BRD4\"\n",
    "    ],\n",
    "    \"Chromatin Structures and Remodelling Factors\": [\n",
    "        \"CHD1\", \"CHD2\", \"CHD4\", \"CHD7\", \"ARID1B\", \"ARID2\", \"ARID3A\", \"ARID3B\",\n",
    "        \"ARID4A\", \"ARID4B\", \"ARID5B\", \"SMARCA4\", \"SMARCB1\"\n",
    "    ],\n",
    "    \"Transcription Factor with Epigenetic Roles\": [\n",
    "        \"CTCF\", \"CTCFL\", \"FOXA1\", \"FOXA2\", \"FOXA3\", \"GATA1\", \"GATA2\", \"GATA3\",\n",
    "        \"NR3C1\", \"ESR1\", \"ETS1\", \"ETS2\"\n",
    "    ]\n",
    "}\n",
    "feature_to_class = {f: cls for cls, features in feature_class_map.items() for f in features}\n",
    "\n",
    "# MySQL connection\n",
    "db = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"B@ndung40175\",\n",
    "    database=\"epidb\"\n",
    ")\n",
    "cursor = db.cursor()\n",
    "\n",
    "csv_folder = \"E:/DataTesis/Final_Epigenomic\"\n",
    "processed_files_path = \"processed_files.txt\"\n",
    "already_processed = set()\n",
    "if os.path.exists(processed_files_path):\n",
    "    with open(processed_files_path, \"r\") as f:\n",
    "        already_processed = set(f.read().splitlines())\n",
    "\n",
    "csv_files = [\n",
    "    f for f in os.listdir(csv_folder)\n",
    "    if f.endswith(\".csv\") and f not in already_processed\n",
    "]\n",
    "\n",
    "# Caches\n",
    "cell_line_cache = {}\n",
    "region_type_cache = {}\n",
    "window_size_cache = {}\n",
    "feature_ids_cache_global = {}\n",
    "region_cache = {}\n",
    "activity_cache = {}\n",
    "\n",
    "# Correct ID column names\n",
    "id_column_map = {\n",
    "    \"CellLine\": \"cell_line_id\",\n",
    "    \"RegionType\": \"region_type_id\",\n",
    "    \"WindowSize\": \"window_size_id\"\n",
    "}\n",
    "\n",
    "def get_or_insert_id(table, column, value, cache):\n",
    "    if value in cache:\n",
    "        return cache[value]\n",
    "\n",
    "    id_column = id_column_map[table]\n",
    "    cursor.execute(f\"SELECT {id_column} FROM {table} WHERE {column} = %s\", (value,))\n",
    "    res = cursor.fetchone()\n",
    "    \n",
    "    if res:\n",
    "        cache[value] = res[0]\n",
    "        return res[0]\n",
    "\n",
    "    cursor.execute(f\"INSERT INTO {table} ({column}) VALUES (%s)\", (value,))\n",
    "    db.commit()  # ✅ Fix: use the correct connection\n",
    "    new_id = cursor.lastrowid\n",
    "    cache[value] = new_id\n",
    "    return new_id\n",
    "\n",
    "for file in tqdm(csv_files, desc=\"Processing CSV files\"):\n",
    "    file_path = os.path.join(csv_folder, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    base_name = os.path.basename(file).replace(\"_epigenomic.csv\", \"\")\n",
    "    cell_line, window_size, region_type = base_name.split(\"_\")\n",
    "    window_size = int(window_size)\n",
    "\n",
    "    cell_line_id = get_or_insert_id(\"CellLine\", \"name\", cell_line, cell_line_cache)\n",
    "    region_type_id = get_or_insert_id(\"RegionType\", \"type\", region_type, region_type_cache)\n",
    "    window_size_id = get_or_insert_id(\"WindowSize\", \"size\", window_size, window_size_cache)\n",
    "\n",
    "    fixed_cols = {\"chrom\", \"start\", \"end\", \"strand\", \"TPM\"}\n",
    "    feature_cols = [col for col in df.columns if col not in fixed_cols]\n",
    "\n",
    "    for feature_name in feature_cols:\n",
    "        if feature_name in feature_ids_cache_global:\n",
    "            continue\n",
    "        feature_class = feature_to_class.get(feature_name, \"Unknown\")\n",
    "        cursor.execute(\n",
    "            \"SELECT feature_id FROM EpigenomicFeature WHERE name = %s AND feature_class = %s\",\n",
    "            (feature_name, feature_class)\n",
    "        )\n",
    "        res = cursor.fetchone()\n",
    "        if res:\n",
    "            feature_ids_cache_global[feature_name] = res[0]\n",
    "        else:\n",
    "            cursor.execute(\n",
    "                \"INSERT INTO EpigenomicFeature (feature_class, name) VALUES (%s, %s)\",\n",
    "                (feature_class, feature_name)\n",
    "            )\n",
    "            db.commit()\n",
    "            feature_ids_cache_global[feature_name] = cursor.lastrowid\n",
    "\n",
    "    feature_value_batch = []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"Inserting rows for {file}\", leave=False):\n",
    "        chrom = row['chrom']\n",
    "        start = int(row['start'])\n",
    "        end = int(row['end'])\n",
    "        strand = row['strand']\n",
    "        tpm = sanitize_value(row['TPM'])\n",
    "\n",
    "        region_key = (chrom, start, end, strand)\n",
    "        if region_key in region_cache:\n",
    "            region_id = region_cache[region_key]\n",
    "        else:\n",
    "            cursor.execute(\n",
    "                \"SELECT region_id FROM GenomicRegion WHERE chrom = %s AND start = %s AND end = %s AND strand = %s\",\n",
    "                region_key\n",
    "            )\n",
    "            res = cursor.fetchone()\n",
    "            if res:\n",
    "                region_id = res[0]\n",
    "            else:\n",
    "                cursor.execute(\n",
    "                    \"INSERT INTO GenomicRegion (chrom, start, end, strand) VALUES (%s, %s, %s, %s)\",\n",
    "                    region_key\n",
    "                )\n",
    "                db.commit()\n",
    "                region_id = cursor.lastrowid\n",
    "            region_cache[region_key] = region_id\n",
    "\n",
    "        activity_key = (region_id, cell_line_id, region_type_id, window_size_id)\n",
    "        if activity_key in activity_cache:\n",
    "            activity_id = activity_cache[activity_key]\n",
    "        else:\n",
    "            cursor.execute(\n",
    "                \"SELECT activity_id FROM EpigenomicActivity WHERE region_id = %s AND cell_line_id = %s AND region_type_id = %s AND window_size_id = %s\",\n",
    "                activity_key\n",
    "            )\n",
    "            res = cursor.fetchone()\n",
    "            if res:\n",
    "                activity_id = res[0]\n",
    "            else:\n",
    "                cursor.execute(\n",
    "                    \"INSERT INTO EpigenomicActivity (region_id, cell_line_id, region_type_id, window_size_id, TPM) VALUES (%s, %s, %s, %s, %s)\",\n",
    "                    (*activity_key, tpm)\n",
    "                )\n",
    "                db.commit()\n",
    "                activity_id = cursor.lastrowid\n",
    "            activity_cache[activity_key] = activity_id\n",
    "\n",
    "        for feature_name in feature_cols:\n",
    "            fold_change = sanitize_value(row[feature_name])\n",
    "            if fold_change is None:\n",
    "                continue\n",
    "            feature_id = feature_ids_cache_global[feature_name]\n",
    "            feature_value_batch.append((activity_id, feature_id, fold_change))\n",
    "\n",
    "        if len(feature_value_batch) >= 1000:\n",
    "            cursor.executemany(\n",
    "                \"INSERT INTO FeatureValue (activity_id, feature_id, fold_change) VALUES (%s, %s, %s)\",\n",
    "                feature_value_batch\n",
    "            )\n",
    "            db.commit()\n",
    "            feature_value_batch = []\n",
    "\n",
    "    if feature_value_batch:\n",
    "        cursor.executemany(\n",
    "            \"INSERT INTO FeatureValue (activity_id, feature_id, fold_change) VALUES (%s, %s, %s)\",\n",
    "            feature_value_batch\n",
    "        )\n",
    "        db.commit()\n",
    "\n",
    "    with open(processed_files_path, \"a\") as f:\n",
    "        f.write(file + \"\\n\")\n",
    "\n",
    "    print(f\"✅ Finished processing: {file}\")\n",
    "\n",
    "cursor.close()\n",
    "db.close()\n",
    "print(\"✅ All files processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "786a2cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  12%|█▎        | 1/8 [3:36:59<25:18:58, 13019.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: HEK293_1024_enhancer_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  25%|██▌       | 2/8 [8:56:56<27:45:20, 16653.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: HEK293_1024_promoter_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  38%|███▊      | 3/8 [9:00:23<12:41:58, 9143.74s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: HepG2_1024_enhancer_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  50%|█████     | 4/8 [9:05:54<6:17:38, 5664.53s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: HepG2_1024_promoter_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  62%|██████▎   | 5/8 [9:09:26<3:04:54, 3698.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: K562_1024_enhancer_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  75%|███████▌  | 6/8 [9:14:58<1:25:07, 2553.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: K562_1024_promoter_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  88%|████████▊ | 7/8 [9:18:05<29:39, 1779.80s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: MCF-7_1024_enhancer_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files: 100%|██████████| 8/8 [9:22:55<00:00, 4221.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: MCF-7_1024_promoter_epigenomic.csv\n",
      "✅ All files processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "from tqdm import tqdm\n",
    "\n",
    "def sanitize_value(val):\n",
    "    if val is None:\n",
    "        return None\n",
    "    if isinstance(val, float) and math.isnan(val):\n",
    "        return None\n",
    "    return val\n",
    "\n",
    "# Feature class map\n",
    "feature_class_map = {\n",
    "    \"Histone Modification\": [\n",
    "        \"H3K27ac\", \"H3K27me3\", \"H3K36me3\", \"H3K4me1\", \"H3K4me2\", \"H3K4me3\",\n",
    "        \"H3K79me2\", \"H3K9ac\", \"H3K9me2\", \"H3K9me3\", \"H4K20me1\"\n",
    "    ],\n",
    "    \"DNA Methylation\": [\"DNMT1\", \"DNMT3B\"],\n",
    "    \"Chromatin Remodeling and Histone Modifiers\": [\n",
    "        \"EZH2\", \"EED\", \"SUZ12\", \"KDM1A\", \"KDM2A\", \"KDM3A\", \"KDM4B\", \"KDM5A\",\n",
    "        \"KDM5B\", \"KDM6A\", \"KAT2A\", \"KAT2B\", \"KAT7\", \"KAT8\", \"HDAC1\", \"HDAC2\",\n",
    "        \"HDAC3\", \"HDAC6\", \"HDAC8\", \"BRD4\"\n",
    "    ],\n",
    "    \"Chromatin Structures and Remodelling Factors\": [\n",
    "        \"CHD1\", \"CHD2\", \"CHD4\", \"CHD7\", \"ARID1B\", \"ARID2\", \"ARID3A\", \"ARID3B\",\n",
    "        \"ARID4A\", \"ARID4B\", \"ARID5B\", \"SMARCA4\", \"SMARCB1\"\n",
    "    ],\n",
    "    \"Transcription Factor with Epigenetic Roles\": [\n",
    "        \"CTCF\", \"CTCFL\", \"FOXA1\", \"FOXA2\", \"FOXA3\", \"GATA1\", \"GATA2\", \"GATA3\",\n",
    "        \"NR3C1\", \"ESR1\", \"ETS1\", \"ETS2\"\n",
    "    ]\n",
    "}\n",
    "feature_to_class = {f: cls for cls, features in feature_class_map.items() for f in features}\n",
    "\n",
    "# MySQL connection\n",
    "db = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"B@ndung40175\",\n",
    "    database=\"epidb\"\n",
    ")\n",
    "cursor = db.cursor()\n",
    "\n",
    "csv_folder = \"E:/DataTesis/Final_Epigenomic\"\n",
    "processed_files_path = \"processed_files.txt\"\n",
    "already_processed = set()\n",
    "if os.path.exists(processed_files_path):\n",
    "    with open(processed_files_path, \"r\") as f:\n",
    "        already_processed = set(f.read().splitlines())\n",
    "\n",
    "csv_files = [\n",
    "    f for f in os.listdir(csv_folder)\n",
    "    if f.endswith(\".csv\") and f not in already_processed\n",
    "]\n",
    "\n",
    "# Caches\n",
    "cell_line_cache = {}\n",
    "region_type_cache = {}\n",
    "window_size_cache = {}\n",
    "feature_ids_cache_global = {}\n",
    "region_cache = {}\n",
    "activity_cache = {}\n",
    "\n",
    "# Correct ID column names\n",
    "id_column_map = {\n",
    "    \"CellLine\": \"cell_line_id\",\n",
    "    \"RegionType\": \"region_type_id\",\n",
    "    \"WindowSize\": \"window_size_id\"\n",
    "}\n",
    "\n",
    "def get_or_insert_id(table, column, value, cache):\n",
    "    if value in cache:\n",
    "        return cache[value]\n",
    "\n",
    "    id_column = id_column_map[table]\n",
    "    cursor.execute(f\"SELECT {id_column} FROM {table} WHERE {column} = %s\", (value,))\n",
    "    res = cursor.fetchone()\n",
    "    \n",
    "    if res:\n",
    "        cache[value] = res[0]\n",
    "        return res[0]\n",
    "\n",
    "    cursor.execute(f\"INSERT INTO {table} ({column}) VALUES (%s)\", (value,))\n",
    "    db.commit()  # ✅ Fix: use the correct connection\n",
    "    new_id = cursor.lastrowid\n",
    "    cache[value] = new_id\n",
    "    return new_id\n",
    "\n",
    "for file in tqdm(csv_files, desc=\"Processing CSV files\"):\n",
    "    file_path = os.path.join(csv_folder, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    base_name = os.path.basename(file).replace(\"_epigenomic.csv\", \"\")\n",
    "    cell_line, window_size, region_type = base_name.split(\"_\")\n",
    "    window_size = int(window_size)\n",
    "\n",
    "    cell_line_id = get_or_insert_id(\"CellLine\", \"name\", cell_line, cell_line_cache)\n",
    "    region_type_id = get_or_insert_id(\"RegionType\", \"type\", region_type, region_type_cache)\n",
    "    window_size_id = get_or_insert_id(\"WindowSize\", \"size\", window_size, window_size_cache)\n",
    "\n",
    "    fixed_cols = {\"chrom\", \"start\", \"end\", \"strand\", \"TPM\"}\n",
    "    feature_cols = [col for col in df.columns if col not in fixed_cols]\n",
    "\n",
    "    for feature_name in feature_cols:\n",
    "        if feature_name in feature_ids_cache_global:\n",
    "            continue\n",
    "        feature_class = feature_to_class.get(feature_name, \"Unknown\")\n",
    "        cursor.execute(\n",
    "            \"SELECT feature_id FROM EpigenomicFeature WHERE name = %s AND feature_class = %s\",\n",
    "            (feature_name, feature_class)\n",
    "        )\n",
    "        res = cursor.fetchone()\n",
    "        if res:\n",
    "            feature_ids_cache_global[feature_name] = res[0]\n",
    "        else:\n",
    "            cursor.execute(\n",
    "                \"INSERT INTO EpigenomicFeature (feature_class, name) VALUES (%s, %s)\",\n",
    "                (feature_class, feature_name)\n",
    "            )\n",
    "            db.commit()\n",
    "            feature_ids_cache_global[feature_name] = cursor.lastrowid\n",
    "\n",
    "    feature_value_batch = []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"Inserting rows for {file}\", leave=False):\n",
    "        chrom = row['chrom']\n",
    "        start = int(row['start'])\n",
    "        end = int(row['end'])\n",
    "        strand = row['strand']\n",
    "        tpm = sanitize_value(row['TPM'])\n",
    "\n",
    "        region_key = (chrom, start, end, strand)\n",
    "        if region_key in region_cache:\n",
    "            region_id = region_cache[region_key]\n",
    "        else:\n",
    "            cursor.execute(\n",
    "                \"SELECT region_id FROM GenomicRegion WHERE chrom = %s AND start = %s AND end = %s AND strand = %s\",\n",
    "                region_key\n",
    "            )\n",
    "            res = cursor.fetchone()\n",
    "            if res:\n",
    "                region_id = res[0]\n",
    "            else:\n",
    "                cursor.execute(\n",
    "                    \"INSERT INTO GenomicRegion (chrom, start, end, strand) VALUES (%s, %s, %s, %s)\",\n",
    "                    region_key\n",
    "                )\n",
    "                db.commit()\n",
    "                region_id = cursor.lastrowid\n",
    "            region_cache[region_key] = region_id\n",
    "\n",
    "        activity_key = (region_id, cell_line_id, region_type_id, window_size_id)\n",
    "        if activity_key in activity_cache:\n",
    "            activity_id = activity_cache[activity_key]\n",
    "        else:\n",
    "            cursor.execute(\n",
    "                \"SELECT activity_id FROM EpigenomicActivity WHERE region_id = %s AND cell_line_id = %s AND region_type_id = %s AND window_size_id = %s\",\n",
    "                activity_key\n",
    "            )\n",
    "            res = cursor.fetchone()\n",
    "            if res:\n",
    "                activity_id = res[0]\n",
    "            else:\n",
    "                cursor.execute(\n",
    "                    \"INSERT INTO EpigenomicActivity (region_id, cell_line_id, region_type_id, window_size_id, TPM) VALUES (%s, %s, %s, %s, %s)\",\n",
    "                    (*activity_key, tpm)\n",
    "                )\n",
    "                db.commit()\n",
    "                activity_id = cursor.lastrowid\n",
    "            activity_cache[activity_key] = activity_id\n",
    "\n",
    "        for feature_name in feature_cols:\n",
    "            fold_change = sanitize_value(row[feature_name])\n",
    "            if fold_change is None:\n",
    "                continue\n",
    "            feature_id = feature_ids_cache_global[feature_name]\n",
    "            feature_value_batch.append((activity_id, feature_id, fold_change))\n",
    "\n",
    "        if len(feature_value_batch) >= 100000:\n",
    "            cursor.executemany(\n",
    "                \"INSERT INTO FeatureValue (activity_id, feature_id, fold_change) VALUES (%s, %s, %s)\",\n",
    "                feature_value_batch\n",
    "            )\n",
    "            db.commit()\n",
    "            feature_value_batch = []\n",
    "\n",
    "    if feature_value_batch:\n",
    "        cursor.executemany(\n",
    "            \"INSERT INTO FeatureValue (activity_id, feature_id, fold_change) VALUES (%s, %s, %s)\",\n",
    "            feature_value_batch\n",
    "        )\n",
    "        db.commit()\n",
    "\n",
    "    with open(processed_files_path, \"a\") as f:\n",
    "        f.write(file + \"\\n\")\n",
    "\n",
    "    print(f\"✅ Finished processing: {file}\")\n",
    "\n",
    "cursor.close()\n",
    "db.close()\n",
    "print(\"✅ All files processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7762241b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:   2%|▏         | 1/48 [3:54:45<183:54:01, 14085.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: GM12878_128_enhancer_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:   4%|▍         | 2/48 [9:35:15<227:40:02, 17817.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: GM12878_128_promoter_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:   6%|▋         | 3/48 [13:16:43<196:52:01, 15749.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: GM12878_256_enhancer_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:   8%|▊         | 4/48 [18:40:48<205:28:53, 16812.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: GM12878_256_promoter_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'E:/DataTesis/Final_Epigenomic\\\\GM12878_512_enhancer_epigenomic.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 93\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m tqdm(csv_files, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing CSV files\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     92\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(csv_folder, file)\n\u001b[1;32m---> 93\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m     base_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(file)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_epigenomic.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     96\u001b[0m     cell_line, window_size, region_type \u001b[38;5;241m=\u001b[39m base_name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'E:/DataTesis/Final_Epigenomic\\\\GM12878_512_enhancer_epigenomic.csv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "from tqdm import tqdm\n",
    "\n",
    "def sanitize_value(val):\n",
    "    if val is None:\n",
    "        return None\n",
    "    if isinstance(val, float) and math.isnan(val):\n",
    "        return None\n",
    "    return val\n",
    "\n",
    "# Feature class map\n",
    "feature_class_map = {\n",
    "    \"Histone Modification\": [\n",
    "        \"H3K27ac\", \"H3K27me3\", \"H3K36me3\", \"H3K4me1\", \"H3K4me2\", \"H3K4me3\",\n",
    "        \"H3K79me2\", \"H3K9ac\", \"H3K9me2\", \"H3K9me3\", \"H4K20me1\"\n",
    "    ],\n",
    "    \"DNA Methylation\": [\"DNMT1\", \"DNMT3B\"],\n",
    "    \"Chromatin Remodeling and Histone Modifiers\": [\n",
    "        \"EZH2\", \"EED\", \"SUZ12\", \"KDM1A\", \"KDM2A\", \"KDM3A\", \"KDM4B\", \"KDM5A\",\n",
    "        \"KDM5B\", \"KDM6A\", \"KAT2A\", \"KAT2B\", \"KAT7\", \"KAT8\", \"HDAC1\", \"HDAC2\",\n",
    "        \"HDAC3\", \"HDAC6\", \"HDAC8\", \"BRD4\"\n",
    "    ],\n",
    "    \"Chromatin Structures and Remodelling Factors\": [\n",
    "        \"CHD1\", \"CHD2\", \"CHD4\", \"CHD7\", \"ARID1B\", \"ARID2\", \"ARID3A\", \"ARID3B\",\n",
    "        \"ARID4A\", \"ARID4B\", \"ARID5B\", \"SMARCA4\", \"SMARCB1\"\n",
    "    ],\n",
    "    \"Transcription Factor with Epigenetic Roles\": [\n",
    "        \"CTCF\", \"CTCFL\", \"FOXA1\", \"FOXA2\", \"FOXA3\", \"GATA1\", \"GATA2\", \"GATA3\",\n",
    "        \"NR3C1\", \"ESR1\", \"ETS1\", \"ETS2\"\n",
    "    ]\n",
    "}\n",
    "feature_to_class = {f: cls for cls, features in feature_class_map.items() for f in features}\n",
    "\n",
    "# MySQL connection\n",
    "db = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"B@ndung40175\",\n",
    "    database=\"epidb\"\n",
    ")\n",
    "cursor = db.cursor()\n",
    "\n",
    "csv_folder = \"E:/DataTesis/Final_Epigenomic\"\n",
    "processed_files_path = \"processed_files.txt\"\n",
    "already_processed = set()\n",
    "if os.path.exists(processed_files_path):\n",
    "    with open(processed_files_path, \"r\") as f:\n",
    "        already_processed = set(f.read().splitlines())\n",
    "\n",
    "csv_files = [\n",
    "    f for f in os.listdir(csv_folder)\n",
    "    if f.endswith(\".csv\") and f not in already_processed\n",
    "]\n",
    "\n",
    "# Caches\n",
    "cell_line_cache = {}\n",
    "region_type_cache = {}\n",
    "window_size_cache = {}\n",
    "feature_ids_cache_global = {}\n",
    "region_cache = {}\n",
    "activity_cache = {}\n",
    "\n",
    "# Correct ID column names\n",
    "id_column_map = {\n",
    "    \"CellLine\": \"cell_line_id\",\n",
    "    \"RegionType\": \"region_type_id\",\n",
    "    \"WindowSize\": \"window_size_id\"\n",
    "}\n",
    "\n",
    "def get_or_insert_id(table, column, value, cache):\n",
    "    if value in cache:\n",
    "        return cache[value]\n",
    "\n",
    "    id_column = id_column_map[table]\n",
    "    cursor.execute(f\"SELECT {id_column} FROM {table} WHERE {column} = %s\", (value,))\n",
    "    res = cursor.fetchone()\n",
    "    \n",
    "    if res:\n",
    "        cache[value] = res[0]\n",
    "        return res[0]\n",
    "\n",
    "    cursor.execute(f\"INSERT INTO {table} ({column}) VALUES (%s)\", (value,))\n",
    "    db.commit()  # ✅ Fix: use the correct connection\n",
    "    new_id = cursor.lastrowid\n",
    "    cache[value] = new_id\n",
    "    return new_id\n",
    "\n",
    "for file in tqdm(csv_files, desc=\"Processing CSV files\"):\n",
    "    file_path = os.path.join(csv_folder, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    base_name = os.path.basename(file).replace(\"_epigenomic.csv\", \"\")\n",
    "    cell_line, window_size, region_type = base_name.split(\"_\")\n",
    "    window_size = int(window_size)\n",
    "\n",
    "    cell_line_id = get_or_insert_id(\"CellLine\", \"name\", cell_line, cell_line_cache)\n",
    "    region_type_id = get_or_insert_id(\"RegionType\", \"type\", region_type, region_type_cache)\n",
    "    window_size_id = get_or_insert_id(\"WindowSize\", \"size\", window_size, window_size_cache)\n",
    "\n",
    "    fixed_cols = {\"chrom\", \"start\", \"end\", \"strand\", \"TPM\"}\n",
    "    feature_cols = [col for col in df.columns if col not in fixed_cols]\n",
    "\n",
    "    for feature_name in feature_cols:\n",
    "        if feature_name in feature_ids_cache_global:\n",
    "            continue\n",
    "        feature_class = feature_to_class.get(feature_name, \"Unknown\")\n",
    "        cursor.execute(\n",
    "            \"SELECT feature_id FROM EpigenomicFeature WHERE name = %s AND feature_class = %s\",\n",
    "            (feature_name, feature_class)\n",
    "        )\n",
    "        res = cursor.fetchone()\n",
    "        if res:\n",
    "            feature_ids_cache_global[feature_name] = res[0]\n",
    "        else:\n",
    "            cursor.execute(\n",
    "                \"INSERT INTO EpigenomicFeature (feature_class, name) VALUES (%s, %s)\",\n",
    "                (feature_class, feature_name)\n",
    "            )\n",
    "            db.commit()\n",
    "            feature_ids_cache_global[feature_name] = cursor.lastrowid\n",
    "\n",
    "    feature_value_batch = []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"Inserting rows for {file}\", leave=False):\n",
    "        chrom = row['chrom']\n",
    "        start = int(row['start'])\n",
    "        end = int(row['end'])\n",
    "        strand = row['strand']\n",
    "        tpm = sanitize_value(row['TPM'])\n",
    "\n",
    "        region_key = (chrom, start, end, strand)\n",
    "        if region_key in region_cache:\n",
    "            region_id = region_cache[region_key]\n",
    "        else:\n",
    "            cursor.execute(\n",
    "                \"SELECT region_id FROM GenomicRegion WHERE chrom = %s AND start = %s AND end = %s AND strand = %s\",\n",
    "                region_key\n",
    "            )\n",
    "            res = cursor.fetchone()\n",
    "            if res:\n",
    "                region_id = res[0]\n",
    "            else:\n",
    "                cursor.execute(\n",
    "                    \"INSERT INTO GenomicRegion (chrom, start, end, strand) VALUES (%s, %s, %s, %s)\",\n",
    "                    region_key\n",
    "                )\n",
    "                db.commit()\n",
    "                region_id = cursor.lastrowid\n",
    "            region_cache[region_key] = region_id\n",
    "\n",
    "        activity_key = (region_id, cell_line_id, region_type_id, window_size_id)\n",
    "        if activity_key in activity_cache:\n",
    "            activity_id = activity_cache[activity_key]\n",
    "        else:\n",
    "            cursor.execute(\n",
    "                \"SELECT activity_id FROM EpigenomicActivity WHERE region_id = %s AND cell_line_id = %s AND region_type_id = %s AND window_size_id = %s\",\n",
    "                activity_key\n",
    "            )\n",
    "            res = cursor.fetchone()\n",
    "            if res:\n",
    "                activity_id = res[0]\n",
    "            else:\n",
    "                cursor.execute(\n",
    "                    \"INSERT INTO EpigenomicActivity (region_id, cell_line_id, region_type_id, window_size_id, TPM) VALUES (%s, %s, %s, %s, %s)\",\n",
    "                    (*activity_key, tpm)\n",
    "                )\n",
    "                db.commit()\n",
    "                activity_id = cursor.lastrowid\n",
    "            activity_cache[activity_key] = activity_id\n",
    "\n",
    "        for feature_name in feature_cols:\n",
    "            fold_change = sanitize_value(row[feature_name])\n",
    "            if fold_change is None:\n",
    "                continue\n",
    "            feature_id = feature_ids_cache_global[feature_name]\n",
    "            feature_value_batch.append((activity_id, feature_id, fold_change))\n",
    "\n",
    "        if len(feature_value_batch) >= 100000:\n",
    "            cursor.executemany(\n",
    "                \"INSERT INTO FeatureValue (activity_id, feature_id, fold_change) VALUES (%s, %s, %s)\",\n",
    "                feature_value_batch\n",
    "            )\n",
    "            db.commit()\n",
    "            feature_value_batch = []\n",
    "\n",
    "    if feature_value_batch:\n",
    "        cursor.executemany(\n",
    "            \"INSERT INTO FeatureValue (activity_id, feature_id, fold_change) VALUES (%s, %s, %s)\",\n",
    "            feature_value_batch\n",
    "        )\n",
    "        db.commit()\n",
    "\n",
    "    with open(processed_files_path, \"a\") as f:\n",
    "        f.write(file + \"\\n\")\n",
    "\n",
    "    print(f\"✅ Finished processing: {file}\")\n",
    "\n",
    "cursor.close()\n",
    "db.close()\n",
    "print(\"✅ All files processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf24f34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  10%|█         | 1/10 [3:20:30<30:04:34, 12030.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: H1_128_enhancer_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  20%|██        | 2/10 [8:41:27<36:10:50, 16281.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: H1_128_promoter_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  30%|███       | 3/10 [8:44:01<17:20:21, 8917.35s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: HEK293_128_enhancer_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  40%|████      | 4/10 [8:48:01<9:09:10, 5491.72s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: HEK293_128_promoter_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  50%|█████     | 5/10 [8:51:27<4:58:46, 3585.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: HepG2_128_enhancer_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  60%|██████    | 6/10 [8:56:49<2:45:03, 2475.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: HepG2_128_promoter_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  70%|███████   | 7/10 [9:00:04<1:26:30, 1730.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: K562_128_enhancer_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  80%|████████  | 8/10 [9:05:08<42:32, 1276.19s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: K562_128_promoter_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  90%|█████████ | 9/10 [9:08:00<15:31, 931.11s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: MCF-7_128_enhancer_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files: 100%|██████████| 10/10 [9:12:34<00:00, 3315.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: MCF-7_128_promoter_epigenomic.csv\n",
      "✅ All files processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "from tqdm import tqdm\n",
    "\n",
    "def sanitize_value(val):\n",
    "    if val is None:\n",
    "        return None\n",
    "    if isinstance(val, float) and math.isnan(val):\n",
    "        return None\n",
    "    return val\n",
    "\n",
    "# Feature class map\n",
    "feature_class_map = {\n",
    "    \"Histone Modification\": [\n",
    "        \"H3K27ac\", \"H3K27me3\", \"H3K36me3\", \"H3K4me1\", \"H3K4me2\", \"H3K4me3\",\n",
    "        \"H3K79me2\", \"H3K9ac\", \"H3K9me2\", \"H3K9me3\", \"H4K20me1\"\n",
    "    ],\n",
    "    \"DNA Methylation\": [\"DNMT1\", \"DNMT3B\"],\n",
    "    \"Chromatin Remodeling and Histone Modifiers\": [\n",
    "        \"EZH2\", \"EED\", \"SUZ12\", \"KDM1A\", \"KDM2A\", \"KDM3A\", \"KDM4B\", \"KDM5A\",\n",
    "        \"KDM5B\", \"KDM6A\", \"KAT2A\", \"KAT2B\", \"KAT7\", \"KAT8\", \"HDAC1\", \"HDAC2\",\n",
    "        \"HDAC3\", \"HDAC6\", \"HDAC8\", \"BRD4\"\n",
    "    ],\n",
    "    \"Chromatin Structures and Remodelling Factors\": [\n",
    "        \"CHD1\", \"CHD2\", \"CHD4\", \"CHD7\", \"ARID1B\", \"ARID2\", \"ARID3A\", \"ARID3B\",\n",
    "        \"ARID4A\", \"ARID4B\", \"ARID5B\", \"SMARCA4\", \"SMARCB1\"\n",
    "    ],\n",
    "    \"Transcription Factor with Epigenetic Roles\": [\n",
    "        \"CTCF\", \"CTCFL\", \"FOXA1\", \"FOXA2\", \"FOXA3\", \"GATA1\", \"GATA2\", \"GATA3\",\n",
    "        \"NR3C1\", \"ESR1\", \"ETS1\", \"ETS2\"\n",
    "    ]\n",
    "}\n",
    "feature_to_class = {f: cls for cls, features in feature_class_map.items() for f in features}\n",
    "\n",
    "# MySQL connection\n",
    "db = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"B@ndung40175\",\n",
    "    database=\"epidb\"\n",
    ")\n",
    "cursor = db.cursor()\n",
    "\n",
    "csv_folder = \"E:/DataTesis/Final_Epigenomic\"\n",
    "processed_files_path = \"processed_files.txt\"\n",
    "already_processed = set()\n",
    "if os.path.exists(processed_files_path):\n",
    "    with open(processed_files_path, \"r\") as f:\n",
    "        already_processed = set(f.read().splitlines())\n",
    "\n",
    "csv_files = [\n",
    "    f for f in os.listdir(csv_folder)\n",
    "    if f.endswith(\".csv\") and f not in already_processed\n",
    "]\n",
    "\n",
    "# Caches\n",
    "cell_line_cache = {}\n",
    "region_type_cache = {}\n",
    "window_size_cache = {}\n",
    "feature_ids_cache_global = {}\n",
    "region_cache = {}\n",
    "activity_cache = {}\n",
    "\n",
    "# Correct ID column names\n",
    "id_column_map = {\n",
    "    \"CellLine\": \"cell_line_id\",\n",
    "    \"RegionType\": \"region_type_id\",\n",
    "    \"WindowSize\": \"window_size_id\"\n",
    "}\n",
    "\n",
    "def get_or_insert_id(table, column, value, cache):\n",
    "    if value in cache:\n",
    "        return cache[value]\n",
    "\n",
    "    id_column = id_column_map[table]\n",
    "    cursor.execute(f\"SELECT {id_column} FROM {table} WHERE {column} = %s\", (value,))\n",
    "    res = cursor.fetchone()\n",
    "    \n",
    "    if res:\n",
    "        cache[value] = res[0]\n",
    "        return res[0]\n",
    "\n",
    "    cursor.execute(f\"INSERT INTO {table} ({column}) VALUES (%s)\", (value,))\n",
    "    db.commit()  # ✅ Fix: use the correct connection\n",
    "    new_id = cursor.lastrowid\n",
    "    cache[value] = new_id\n",
    "    return new_id\n",
    "\n",
    "for file in tqdm(csv_files, desc=\"Processing CSV files\"):\n",
    "    file_path = os.path.join(csv_folder, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    base_name = os.path.basename(file).replace(\"_epigenomic.csv\", \"\")\n",
    "    cell_line, window_size, region_type = base_name.split(\"_\")\n",
    "    window_size = int(window_size)\n",
    "\n",
    "    cell_line_id = get_or_insert_id(\"CellLine\", \"name\", cell_line, cell_line_cache)\n",
    "    region_type_id = get_or_insert_id(\"RegionType\", \"type\", region_type, region_type_cache)\n",
    "    window_size_id = get_or_insert_id(\"WindowSize\", \"size\", window_size, window_size_cache)\n",
    "\n",
    "    fixed_cols = {\"chrom\", \"start\", \"end\", \"strand\", \"TPM\"}\n",
    "    feature_cols = [col for col in df.columns if col not in fixed_cols]\n",
    "\n",
    "    for feature_name in feature_cols:\n",
    "        if feature_name in feature_ids_cache_global:\n",
    "            continue\n",
    "        feature_class = feature_to_class.get(feature_name, \"Unknown\")\n",
    "        cursor.execute(\n",
    "            \"SELECT feature_id FROM EpigenomicFeature WHERE name = %s AND feature_class = %s\",\n",
    "            (feature_name, feature_class)\n",
    "        )\n",
    "        res = cursor.fetchone()\n",
    "        if res:\n",
    "            feature_ids_cache_global[feature_name] = res[0]\n",
    "        else:\n",
    "            cursor.execute(\n",
    "                \"INSERT INTO EpigenomicFeature (feature_class, name) VALUES (%s, %s)\",\n",
    "                (feature_class, feature_name)\n",
    "            )\n",
    "            db.commit()\n",
    "            feature_ids_cache_global[feature_name] = cursor.lastrowid\n",
    "\n",
    "    feature_value_batch = []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"Inserting rows for {file}\", leave=False):\n",
    "        chrom = row['chrom']\n",
    "        start = int(row['start'])\n",
    "        end = int(row['end'])\n",
    "        strand = row['strand']\n",
    "        tpm = sanitize_value(row['TPM'])\n",
    "\n",
    "        region_key = (chrom, start, end, strand)\n",
    "        if region_key in region_cache:\n",
    "            region_id = region_cache[region_key]\n",
    "        else:\n",
    "            cursor.execute(\n",
    "                \"SELECT region_id FROM GenomicRegion WHERE chrom = %s AND start = %s AND end = %s AND strand = %s\",\n",
    "                region_key\n",
    "            )\n",
    "            res = cursor.fetchone()\n",
    "            if res:\n",
    "                region_id = res[0]\n",
    "            else:\n",
    "                cursor.execute(\n",
    "                    \"INSERT INTO GenomicRegion (chrom, start, end, strand) VALUES (%s, %s, %s, %s)\",\n",
    "                    region_key\n",
    "                )\n",
    "                db.commit()\n",
    "                region_id = cursor.lastrowid\n",
    "            region_cache[region_key] = region_id\n",
    "\n",
    "        activity_key = (region_id, cell_line_id, region_type_id, window_size_id)\n",
    "        if activity_key in activity_cache:\n",
    "            activity_id = activity_cache[activity_key]\n",
    "        else:\n",
    "            cursor.execute(\n",
    "                \"SELECT activity_id FROM EpigenomicActivity WHERE region_id = %s AND cell_line_id = %s AND region_type_id = %s AND window_size_id = %s\",\n",
    "                activity_key\n",
    "            )\n",
    "            res = cursor.fetchone()\n",
    "            if res:\n",
    "                activity_id = res[0]\n",
    "            else:\n",
    "                cursor.execute(\n",
    "                    \"INSERT INTO EpigenomicActivity (region_id, cell_line_id, region_type_id, window_size_id, TPM) VALUES (%s, %s, %s, %s, %s)\",\n",
    "                    (*activity_key, tpm)\n",
    "                )\n",
    "                db.commit()\n",
    "                activity_id = cursor.lastrowid\n",
    "            activity_cache[activity_key] = activity_id\n",
    "\n",
    "        for feature_name in feature_cols:\n",
    "            fold_change = sanitize_value(row[feature_name])\n",
    "            if fold_change is None:\n",
    "                continue\n",
    "            feature_id = feature_ids_cache_global[feature_name]\n",
    "            feature_value_batch.append((activity_id, feature_id, fold_change))\n",
    "\n",
    "        if len(feature_value_batch) >= 100000:\n",
    "            cursor.executemany(\n",
    "                \"INSERT INTO FeatureValue (activity_id, feature_id, fold_change) VALUES (%s, %s, %s)\",\n",
    "                feature_value_batch\n",
    "            )\n",
    "            db.commit()\n",
    "            feature_value_batch = []\n",
    "\n",
    "    if feature_value_batch:\n",
    "        cursor.executemany(\n",
    "            \"INSERT INTO FeatureValue (activity_id, feature_id, fold_change) VALUES (%s, %s, %s)\",\n",
    "            feature_value_batch\n",
    "        )\n",
    "        db.commit()\n",
    "\n",
    "    with open(processed_files_path, \"a\") as f:\n",
    "        f.write(file + \"\\n\")\n",
    "\n",
    "    print(f\"✅ Finished processing: {file}\")\n",
    "\n",
    "cursor.close()\n",
    "db.close()\n",
    "print(\"✅ All files processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a7a3c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:   8%|▊         | 1/12 [3:28:51<38:17:26, 12531.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: GM12878_64_enhancer_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  17%|█▋        | 2/12 [8:36:23<44:28:59, 16013.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: GM12878_64_promoter_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  25%|██▌       | 3/12 [8:39:00<21:56:00, 8773.38s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: H1_64_enhancer_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  33%|███▎      | 4/12 [8:43:10<12:01:08, 5408.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: H1_64_promoter_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  42%|████▏     | 5/12 [8:45:34<6:49:31, 3510.27s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: HEK293_64_enhancer_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  50%|█████     | 6/12 [8:49:18<3:59:17, 2392.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: HEK293_64_promoter_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  58%|█████▊    | 7/12 [8:52:27<2:19:21, 1672.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: HepG2_64_enhancer_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  67%|██████▋   | 8/12 [8:57:30<1:22:25, 1236.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: HepG2_64_promoter_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  75%|███████▌  | 9/12 [9:00:35<45:23, 907.72s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: K562_64_enhancer_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  83%|████████▎ | 10/12 [9:05:28<23:55, 717.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: K562_64_promoter_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  92%|█████████▏| 11/12 [9:08:11<09:08, 548.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: MCF-7_64_enhancer_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files: 100%|██████████| 12/12 [9:12:24<00:00, 2762.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: MCF-7_64_promoter_epigenomic.csv\n",
      "✅ All files processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "from tqdm import tqdm\n",
    "\n",
    "def sanitize_value(val):\n",
    "    if val is None:\n",
    "        return None\n",
    "    if isinstance(val, float) and math.isnan(val):\n",
    "        return None\n",
    "    return val\n",
    "\n",
    "# Feature class map\n",
    "feature_class_map = {\n",
    "    \"Histone Modification\": [\n",
    "        \"H3K27ac\", \"H3K27me3\", \"H3K36me3\", \"H3K4me1\", \"H3K4me2\", \"H3K4me3\",\n",
    "        \"H3K79me2\", \"H3K9ac\", \"H3K9me2\", \"H3K9me3\", \"H4K20me1\"\n",
    "    ],\n",
    "    \"DNA Methylation\": [\"DNMT1\", \"DNMT3B\"],\n",
    "    \"Chromatin Remodeling and Histone Modifiers\": [\n",
    "        \"EZH2\", \"EED\", \"SUZ12\", \"KDM1A\", \"KDM2A\", \"KDM3A\", \"KDM4B\", \"KDM5A\",\n",
    "        \"KDM5B\", \"KDM6A\", \"KAT2A\", \"KAT2B\", \"KAT7\", \"KAT8\", \"HDAC1\", \"HDAC2\",\n",
    "        \"HDAC3\", \"HDAC6\", \"HDAC8\", \"BRD4\"\n",
    "    ],\n",
    "    \"Chromatin Structures and Remodelling Factors\": [\n",
    "        \"CHD1\", \"CHD2\", \"CHD4\", \"CHD7\", \"ARID1B\", \"ARID2\", \"ARID3A\", \"ARID3B\",\n",
    "        \"ARID4A\", \"ARID4B\", \"ARID5B\", \"SMARCA4\", \"SMARCB1\"\n",
    "    ],\n",
    "    \"Transcription Factor with Epigenetic Roles\": [\n",
    "        \"CTCF\", \"CTCFL\", \"FOXA1\", \"FOXA2\", \"FOXA3\", \"GATA1\", \"GATA2\", \"GATA3\",\n",
    "        \"NR3C1\", \"ESR1\", \"ETS1\", \"ETS2\"\n",
    "    ]\n",
    "}\n",
    "feature_to_class = {f: cls for cls, features in feature_class_map.items() for f in features}\n",
    "\n",
    "# MySQL connection\n",
    "db = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"B@ndung40175\",\n",
    "    database=\"epidb\"\n",
    ")\n",
    "cursor = db.cursor()\n",
    "\n",
    "csv_folder = \"E:/DataTesis/Final_Epigenomic\"\n",
    "processed_files_path = \"processed_files.txt\"\n",
    "already_processed = set()\n",
    "if os.path.exists(processed_files_path):\n",
    "    with open(processed_files_path, \"r\") as f:\n",
    "        already_processed = set(f.read().splitlines())\n",
    "\n",
    "csv_files = [\n",
    "    f for f in os.listdir(csv_folder)\n",
    "    if f.endswith(\".csv\") and f not in already_processed\n",
    "]\n",
    "\n",
    "# Caches\n",
    "cell_line_cache = {}\n",
    "region_type_cache = {}\n",
    "window_size_cache = {}\n",
    "feature_ids_cache_global = {}\n",
    "region_cache = {}\n",
    "activity_cache = {}\n",
    "\n",
    "# Correct ID column names\n",
    "id_column_map = {\n",
    "    \"CellLine\": \"cell_line_id\",\n",
    "    \"RegionType\": \"region_type_id\",\n",
    "    \"WindowSize\": \"window_size_id\"\n",
    "}\n",
    "\n",
    "def get_or_insert_id(table, column, value, cache):\n",
    "    if value in cache:\n",
    "        return cache[value]\n",
    "\n",
    "    id_column = id_column_map[table]\n",
    "    cursor.execute(f\"SELECT {id_column} FROM {table} WHERE {column} = %s\", (value,))\n",
    "    res = cursor.fetchone()\n",
    "    \n",
    "    if res:\n",
    "        cache[value] = res[0]\n",
    "        return res[0]\n",
    "\n",
    "    cursor.execute(f\"INSERT INTO {table} ({column}) VALUES (%s)\", (value,))\n",
    "    db.commit()  # ✅ Fix: use the correct connection\n",
    "    new_id = cursor.lastrowid\n",
    "    cache[value] = new_id\n",
    "    return new_id\n",
    "\n",
    "for file in tqdm(csv_files, desc=\"Processing CSV files\"):\n",
    "    file_path = os.path.join(csv_folder, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    base_name = os.path.basename(file).replace(\"_epigenomic.csv\", \"\")\n",
    "    cell_line, window_size, region_type = base_name.split(\"_\")\n",
    "    window_size = int(window_size)\n",
    "\n",
    "    cell_line_id = get_or_insert_id(\"CellLine\", \"name\", cell_line, cell_line_cache)\n",
    "    region_type_id = get_or_insert_id(\"RegionType\", \"type\", region_type, region_type_cache)\n",
    "    window_size_id = get_or_insert_id(\"WindowSize\", \"size\", window_size, window_size_cache)\n",
    "\n",
    "    fixed_cols = {\"chrom\", \"start\", \"end\", \"strand\", \"TPM\"}\n",
    "    feature_cols = [col for col in df.columns if col not in fixed_cols]\n",
    "\n",
    "    for feature_name in feature_cols:\n",
    "        if feature_name in feature_ids_cache_global:\n",
    "            continue\n",
    "        feature_class = feature_to_class.get(feature_name, \"Unknown\")\n",
    "        cursor.execute(\n",
    "            \"SELECT feature_id FROM EpigenomicFeature WHERE name = %s AND feature_class = %s\",\n",
    "            (feature_name, feature_class)\n",
    "        )\n",
    "        res = cursor.fetchone()\n",
    "        if res:\n",
    "            feature_ids_cache_global[feature_name] = res[0]\n",
    "        else:\n",
    "            cursor.execute(\n",
    "                \"INSERT INTO EpigenomicFeature (feature_class, name) VALUES (%s, %s)\",\n",
    "                (feature_class, feature_name)\n",
    "            )\n",
    "            db.commit()\n",
    "            feature_ids_cache_global[feature_name] = cursor.lastrowid\n",
    "\n",
    "    feature_value_batch = []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"Inserting rows for {file}\", leave=False):\n",
    "        chrom = row['chrom']\n",
    "        start = int(row['start'])\n",
    "        end = int(row['end'])\n",
    "        strand = row['strand']\n",
    "        tpm = sanitize_value(row['TPM'])\n",
    "\n",
    "        region_key = (chrom, start, end, strand)\n",
    "        if region_key in region_cache:\n",
    "            region_id = region_cache[region_key]\n",
    "        else:\n",
    "            cursor.execute(\n",
    "                \"SELECT region_id FROM GenomicRegion WHERE chrom = %s AND start = %s AND end = %s AND strand = %s\",\n",
    "                region_key\n",
    "            )\n",
    "            res = cursor.fetchone()\n",
    "            if res:\n",
    "                region_id = res[0]\n",
    "            else:\n",
    "                cursor.execute(\n",
    "                    \"INSERT INTO GenomicRegion (chrom, start, end, strand) VALUES (%s, %s, %s, %s)\",\n",
    "                    region_key\n",
    "                )\n",
    "                db.commit()\n",
    "                region_id = cursor.lastrowid\n",
    "            region_cache[region_key] = region_id\n",
    "\n",
    "        activity_key = (region_id, cell_line_id, region_type_id, window_size_id)\n",
    "        if activity_key in activity_cache:\n",
    "            activity_id = activity_cache[activity_key]\n",
    "        else:\n",
    "            cursor.execute(\n",
    "                \"SELECT activity_id FROM EpigenomicActivity WHERE region_id = %s AND cell_line_id = %s AND region_type_id = %s AND window_size_id = %s\",\n",
    "                activity_key\n",
    "            )\n",
    "            res = cursor.fetchone()\n",
    "            if res:\n",
    "                activity_id = res[0]\n",
    "            else:\n",
    "                cursor.execute(\n",
    "                    \"INSERT INTO EpigenomicActivity (region_id, cell_line_id, region_type_id, window_size_id, TPM) VALUES (%s, %s, %s, %s, %s)\",\n",
    "                    (*activity_key, tpm)\n",
    "                )\n",
    "                db.commit()\n",
    "                activity_id = cursor.lastrowid\n",
    "            activity_cache[activity_key] = activity_id\n",
    "\n",
    "        for feature_name in feature_cols:\n",
    "            fold_change = sanitize_value(row[feature_name])\n",
    "            if fold_change is None:\n",
    "                continue\n",
    "            feature_id = feature_ids_cache_global[feature_name]\n",
    "            feature_value_batch.append((activity_id, feature_id, fold_change))\n",
    "\n",
    "        if len(feature_value_batch) >= 100000:\n",
    "            cursor.executemany(\n",
    "                \"INSERT INTO FeatureValue (activity_id, feature_id, fold_change) VALUES (%s, %s, %s)\",\n",
    "                feature_value_batch\n",
    "            )\n",
    "            db.commit()\n",
    "            feature_value_batch = []\n",
    "\n",
    "    if feature_value_batch:\n",
    "        cursor.executemany(\n",
    "            \"INSERT INTO FeatureValue (activity_id, feature_id, fold_change) VALUES (%s, %s, %s)\",\n",
    "            feature_value_batch\n",
    "        )\n",
    "        db.commit()\n",
    "\n",
    "    with open(processed_files_path, \"a\") as f:\n",
    "        f.write(file + \"\\n\")\n",
    "\n",
    "    print(f\"✅ Finished processing: {file}\")\n",
    "\n",
    "cursor.close()\n",
    "db.close()\n",
    "print(\"✅ All files processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0fb0ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:   8%|▊         | 1/12 [3:41:52<40:40:32, 13312.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: GM12878_512_enhancer_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  17%|█▋        | 2/12 [9:18:57<48:16:24, 17378.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: GM12878_512_promoter_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  25%|██▌       | 3/12 [9:21:52<23:48:30, 9523.44s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: H1_512_enhancer_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  33%|███▎      | 4/12 [9:26:33<13:03:14, 5874.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: H1_512_promoter_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  42%|████▏     | 5/12 [9:29:20<7:25:15, 3816.45s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: HEK293_512_enhancer_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  50%|█████     | 6/12 [9:33:34<4:20:31, 2605.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: HEK293_512_promoter_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  58%|█████▊    | 7/12 [9:37:04<2:31:50, 1822.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: HepG2_512_enhancer_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  67%|██████▋   | 8/12 [9:42:35<1:29:49, 1347.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: HepG2_512_promoter_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  75%|███████▌  | 9/12 [9:45:58<49:28, 989.58s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: K562_512_enhancer_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  83%|████████▎ | 10/12 [9:51:24<26:09, 784.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: K562_512_promoter_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  92%|█████████▏| 11/12 [9:54:22<09:59, 599.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: MCF-7_512_enhancer_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files: 100%|██████████| 12/12 [9:59:05<00:00, 2995.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: MCF-7_512_promoter_epigenomic.csv\n",
      "✅ All files processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "from tqdm import tqdm\n",
    "\n",
    "def sanitize_value(val):\n",
    "    if val is None:\n",
    "        return None\n",
    "    if isinstance(val, float) and math.isnan(val):\n",
    "        return None\n",
    "    return val\n",
    "\n",
    "# Feature class map\n",
    "feature_class_map = {\n",
    "    \"Histone Modification\": [\n",
    "        \"H3K27ac\", \"H3K27me3\", \"H3K36me3\", \"H3K4me1\", \"H3K4me2\", \"H3K4me3\",\n",
    "        \"H3K79me2\", \"H3K9ac\", \"H3K9me2\", \"H3K9me3\", \"H4K20me1\"\n",
    "    ],\n",
    "    \"DNA Methylation\": [\"DNMT1\", \"DNMT3B\"],\n",
    "    \"Chromatin Remodeling and Histone Modifiers\": [\n",
    "        \"EZH2\", \"EED\", \"SUZ12\", \"KDM1A\", \"KDM2A\", \"KDM3A\", \"KDM4B\", \"KDM5A\",\n",
    "        \"KDM5B\", \"KDM6A\", \"KAT2A\", \"KAT2B\", \"KAT7\", \"KAT8\", \"HDAC1\", \"HDAC2\",\n",
    "        \"HDAC3\", \"HDAC6\", \"HDAC8\", \"BRD4\"\n",
    "    ],\n",
    "    \"Chromatin Structures and Remodelling Factors\": [\n",
    "        \"CHD1\", \"CHD2\", \"CHD4\", \"CHD7\", \"ARID1B\", \"ARID2\", \"ARID3A\", \"ARID3B\",\n",
    "        \"ARID4A\", \"ARID4B\", \"ARID5B\", \"SMARCA4\", \"SMARCB1\"\n",
    "    ],\n",
    "    \"Transcription Factor with Epigenetic Roles\": [\n",
    "        \"CTCF\", \"CTCFL\", \"FOXA1\", \"FOXA2\", \"FOXA3\", \"GATA1\", \"GATA2\", \"GATA3\",\n",
    "        \"NR3C1\", \"ESR1\", \"ETS1\", \"ETS2\"\n",
    "    ]\n",
    "}\n",
    "feature_to_class = {f: cls for cls, features in feature_class_map.items() for f in features}\n",
    "\n",
    "# MySQL connection\n",
    "db = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"B@ndung40175\",\n",
    "    database=\"epidb\"\n",
    ")\n",
    "cursor = db.cursor()\n",
    "\n",
    "csv_folder = \"E:/DataTesis/Final_Epigenomic\"\n",
    "processed_files_path = \"processed_files.txt\"\n",
    "already_processed = set()\n",
    "if os.path.exists(processed_files_path):\n",
    "    with open(processed_files_path, \"r\") as f:\n",
    "        already_processed = set(f.read().splitlines())\n",
    "\n",
    "csv_files = [\n",
    "    f for f in os.listdir(csv_folder)\n",
    "    if f.endswith(\".csv\") and f not in already_processed\n",
    "]\n",
    "\n",
    "# Caches\n",
    "cell_line_cache = {}\n",
    "region_type_cache = {}\n",
    "window_size_cache = {}\n",
    "feature_ids_cache_global = {}\n",
    "region_cache = {}\n",
    "activity_cache = {}\n",
    "\n",
    "# Correct ID column names\n",
    "id_column_map = {\n",
    "    \"CellLine\": \"cell_line_id\",\n",
    "    \"RegionType\": \"region_type_id\",\n",
    "    \"WindowSize\": \"window_size_id\"\n",
    "}\n",
    "\n",
    "def get_or_insert_id(table, column, value, cache):\n",
    "    if value in cache:\n",
    "        return cache[value]\n",
    "\n",
    "    id_column = id_column_map[table]\n",
    "    cursor.execute(f\"SELECT {id_column} FROM {table} WHERE {column} = %s\", (value,))\n",
    "    res = cursor.fetchone()\n",
    "    \n",
    "    if res:\n",
    "        cache[value] = res[0]\n",
    "        return res[0]\n",
    "\n",
    "    cursor.execute(f\"INSERT INTO {table} ({column}) VALUES (%s)\", (value,))\n",
    "    db.commit()  # ✅ Fix: use the correct connection\n",
    "    new_id = cursor.lastrowid\n",
    "    cache[value] = new_id\n",
    "    return new_id\n",
    "\n",
    "for file in tqdm(csv_files, desc=\"Processing CSV files\"):\n",
    "    file_path = os.path.join(csv_folder, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    base_name = os.path.basename(file).replace(\"_epigenomic.csv\", \"\")\n",
    "    cell_line, window_size, region_type = base_name.split(\"_\")\n",
    "    window_size = int(window_size)\n",
    "\n",
    "    cell_line_id = get_or_insert_id(\"CellLine\", \"name\", cell_line, cell_line_cache)\n",
    "    region_type_id = get_or_insert_id(\"RegionType\", \"type\", region_type, region_type_cache)\n",
    "    window_size_id = get_or_insert_id(\"WindowSize\", \"size\", window_size, window_size_cache)\n",
    "\n",
    "    fixed_cols = {\"chrom\", \"start\", \"end\", \"strand\", \"TPM\"}\n",
    "    feature_cols = [col for col in df.columns if col not in fixed_cols]\n",
    "\n",
    "    for feature_name in feature_cols:\n",
    "        if feature_name in feature_ids_cache_global:\n",
    "            continue\n",
    "        feature_class = feature_to_class.get(feature_name, \"Unknown\")\n",
    "        cursor.execute(\n",
    "            \"SELECT feature_id FROM EpigenomicFeature WHERE name = %s AND feature_class = %s\",\n",
    "            (feature_name, feature_class)\n",
    "        )\n",
    "        res = cursor.fetchone()\n",
    "        if res:\n",
    "            feature_ids_cache_global[feature_name] = res[0]\n",
    "        else:\n",
    "            cursor.execute(\n",
    "                \"INSERT INTO EpigenomicFeature (feature_class, name) VALUES (%s, %s)\",\n",
    "                (feature_class, feature_name)\n",
    "            )\n",
    "            db.commit()\n",
    "            feature_ids_cache_global[feature_name] = cursor.lastrowid\n",
    "\n",
    "    feature_value_batch = []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"Inserting rows for {file}\", leave=False):\n",
    "        chrom = row['chrom']\n",
    "        start = int(row['start'])\n",
    "        end = int(row['end'])\n",
    "        strand = row['strand']\n",
    "        tpm = sanitize_value(row['TPM'])\n",
    "\n",
    "        region_key = (chrom, start, end, strand)\n",
    "        if region_key in region_cache:\n",
    "            region_id = region_cache[region_key]\n",
    "        else:\n",
    "            cursor.execute(\n",
    "                \"SELECT region_id FROM GenomicRegion WHERE chrom = %s AND start = %s AND end = %s AND strand = %s\",\n",
    "                region_key\n",
    "            )\n",
    "            res = cursor.fetchone()\n",
    "            if res:\n",
    "                region_id = res[0]\n",
    "            else:\n",
    "                cursor.execute(\n",
    "                    \"INSERT INTO GenomicRegion (chrom, start, end, strand) VALUES (%s, %s, %s, %s)\",\n",
    "                    region_key\n",
    "                )\n",
    "                db.commit()\n",
    "                region_id = cursor.lastrowid\n",
    "            region_cache[region_key] = region_id\n",
    "\n",
    "        activity_key = (region_id, cell_line_id, region_type_id, window_size_id)\n",
    "        if activity_key in activity_cache:\n",
    "            activity_id = activity_cache[activity_key]\n",
    "        else:\n",
    "            cursor.execute(\n",
    "                \"SELECT activity_id FROM EpigenomicActivity WHERE region_id = %s AND cell_line_id = %s AND region_type_id = %s AND window_size_id = %s\",\n",
    "                activity_key\n",
    "            )\n",
    "            res = cursor.fetchone()\n",
    "            if res:\n",
    "                activity_id = res[0]\n",
    "            else:\n",
    "                cursor.execute(\n",
    "                    \"INSERT INTO EpigenomicActivity (region_id, cell_line_id, region_type_id, window_size_id, TPM) VALUES (%s, %s, %s, %s, %s)\",\n",
    "                    (*activity_key, tpm)\n",
    "                )\n",
    "                db.commit()\n",
    "                activity_id = cursor.lastrowid\n",
    "            activity_cache[activity_key] = activity_id\n",
    "\n",
    "        for feature_name in feature_cols:\n",
    "            fold_change = sanitize_value(row[feature_name])\n",
    "            if fold_change is None:\n",
    "                continue\n",
    "            feature_id = feature_ids_cache_global[feature_name]\n",
    "            feature_value_batch.append((activity_id, feature_id, fold_change))\n",
    "\n",
    "        if len(feature_value_batch) >= 100000:\n",
    "            cursor.executemany(\n",
    "                \"INSERT INTO FeatureValue (activity_id, feature_id, fold_change) VALUES (%s, %s, %s)\",\n",
    "                feature_value_batch\n",
    "            )\n",
    "            db.commit()\n",
    "            feature_value_batch = []\n",
    "\n",
    "    if feature_value_batch:\n",
    "        cursor.executemany(\n",
    "            \"INSERT INTO FeatureValue (activity_id, feature_id, fold_change) VALUES (%s, %s, %s)\",\n",
    "            feature_value_batch\n",
    "        )\n",
    "        db.commit()\n",
    "\n",
    "    with open(processed_files_path, \"a\") as f:\n",
    "        f.write(file + \"\\n\")\n",
    "\n",
    "    print(f\"✅ Finished processing: {file}\")\n",
    "\n",
    "cursor.close()\n",
    "db.close()\n",
    "print(\"✅ All files processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "210faeaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  10%|█         | 1/10 [3:41:30<33:13:38, 13290.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: H1_256_enhancer_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  20%|██        | 2/10 [8:47:53<36:11:27, 16285.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: H1_256_promoter_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  30%|███       | 3/10 [8:50:18<17:20:08, 8915.52s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: HEK293_256_enhancer_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  40%|████      | 4/10 [8:53:59<9:08:19, 5483.31s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: HEK293_256_promoter_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  50%|█████     | 5/10 [8:57:06<4:57:45, 3573.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: HepG2_256_enhancer_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  60%|██████    | 6/10 [9:02:01<2:43:54, 2458.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: HepG2_256_promoter_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  70%|███████   | 7/10 [9:05:05<1:25:44, 1714.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: K562_256_enhancer_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  80%|████████  | 8/10 [9:09:56<42:03, 1261.86s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: K562_256_promoter_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  90%|█████████ | 9/10 [9:12:36<15:17, 917.32s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: MCF-7_256_enhancer_epigenomic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files: 100%|██████████| 10/10 [9:16:50<00:00, 3341.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished processing: MCF-7_256_promoter_epigenomic.csv\n",
      "✅ All files processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "from tqdm import tqdm\n",
    "\n",
    "def sanitize_value(val):\n",
    "    if val is None:\n",
    "        return None\n",
    "    if isinstance(val, float) and math.isnan(val):\n",
    "        return None\n",
    "    return val\n",
    "\n",
    "# Feature class map\n",
    "feature_class_map = {\n",
    "    \"Histone Modification\": [\n",
    "        \"H3K27ac\", \"H3K27me3\", \"H3K36me3\", \"H3K4me1\", \"H3K4me2\", \"H3K4me3\",\n",
    "        \"H3K79me2\", \"H3K9ac\", \"H3K9me2\", \"H3K9me3\", \"H4K20me1\"\n",
    "    ],\n",
    "    \"DNA Methylation\": [\"DNMT1\", \"DNMT3B\"],\n",
    "    \"Chromatin Remodeling and Histone Modifiers\": [\n",
    "        \"EZH2\", \"EED\", \"SUZ12\", \"KDM1A\", \"KDM2A\", \"KDM3A\", \"KDM4B\", \"KDM5A\",\n",
    "        \"KDM5B\", \"KDM6A\", \"KAT2A\", \"KAT2B\", \"KAT7\", \"KAT8\", \"HDAC1\", \"HDAC2\",\n",
    "        \"HDAC3\", \"HDAC6\", \"HDAC8\", \"BRD4\"\n",
    "    ],\n",
    "    \"Chromatin Structures and Remodelling Factors\": [\n",
    "        \"CHD1\", \"CHD2\", \"CHD4\", \"CHD7\", \"ARID1B\", \"ARID2\", \"ARID3A\", \"ARID3B\",\n",
    "        \"ARID4A\", \"ARID4B\", \"ARID5B\", \"SMARCA4\", \"SMARCB1\"\n",
    "    ],\n",
    "    \"Transcription Factor with Epigenetic Roles\": [\n",
    "        \"CTCF\", \"CTCFL\", \"FOXA1\", \"FOXA2\", \"FOXA3\", \"GATA1\", \"GATA2\", \"GATA3\",\n",
    "        \"NR3C1\", \"ESR1\", \"ETS1\", \"ETS2\"\n",
    "    ]\n",
    "}\n",
    "feature_to_class = {f: cls for cls, features in feature_class_map.items() for f in features}\n",
    "\n",
    "# MySQL connection\n",
    "db = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"B@ndung40175\",\n",
    "    database=\"epidb\"\n",
    ")\n",
    "cursor = db.cursor()\n",
    "\n",
    "csv_folder = \"E:/DataTesis/Final_Epigenomic\"\n",
    "processed_files_path = \"processed_files.txt\"\n",
    "already_processed = set()\n",
    "if os.path.exists(processed_files_path):\n",
    "    with open(processed_files_path, \"r\") as f:\n",
    "        already_processed = set(f.read().splitlines())\n",
    "\n",
    "csv_files = [\n",
    "    f for f in os.listdir(csv_folder)\n",
    "    if f.endswith(\".csv\") and f not in already_processed\n",
    "]\n",
    "\n",
    "# Caches\n",
    "cell_line_cache = {}\n",
    "region_type_cache = {}\n",
    "window_size_cache = {}\n",
    "feature_ids_cache_global = {}\n",
    "region_cache = {}\n",
    "activity_cache = {}\n",
    "\n",
    "# Correct ID column names\n",
    "id_column_map = {\n",
    "    \"CellLine\": \"cell_line_id\",\n",
    "    \"RegionType\": \"region_type_id\",\n",
    "    \"WindowSize\": \"window_size_id\"\n",
    "}\n",
    "\n",
    "def get_or_insert_id(table, column, value, cache):\n",
    "    if value in cache:\n",
    "        return cache[value]\n",
    "\n",
    "    id_column = id_column_map[table]\n",
    "    cursor.execute(f\"SELECT {id_column} FROM {table} WHERE {column} = %s\", (value,))\n",
    "    res = cursor.fetchone()\n",
    "    \n",
    "    if res:\n",
    "        cache[value] = res[0]\n",
    "        return res[0]\n",
    "\n",
    "    cursor.execute(f\"INSERT INTO {table} ({column}) VALUES (%s)\", (value,))\n",
    "    db.commit()  # ✅ Fix: use the correct connection\n",
    "    new_id = cursor.lastrowid\n",
    "    cache[value] = new_id\n",
    "    return new_id\n",
    "\n",
    "for file in tqdm(csv_files, desc=\"Processing CSV files\"):\n",
    "    file_path = os.path.join(csv_folder, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    base_name = os.path.basename(file).replace(\"_epigenomic.csv\", \"\")\n",
    "    cell_line, window_size, region_type = base_name.split(\"_\")\n",
    "    window_size = int(window_size)\n",
    "\n",
    "    cell_line_id = get_or_insert_id(\"CellLine\", \"name\", cell_line, cell_line_cache)\n",
    "    region_type_id = get_or_insert_id(\"RegionType\", \"type\", region_type, region_type_cache)\n",
    "    window_size_id = get_or_insert_id(\"WindowSize\", \"size\", window_size, window_size_cache)\n",
    "\n",
    "    fixed_cols = {\"chrom\", \"start\", \"end\", \"strand\", \"TPM\"}\n",
    "    feature_cols = [col for col in df.columns if col not in fixed_cols]\n",
    "\n",
    "    for feature_name in feature_cols:\n",
    "        if feature_name in feature_ids_cache_global:\n",
    "            continue\n",
    "        feature_class = feature_to_class.get(feature_name, \"Unknown\")\n",
    "        cursor.execute(\n",
    "            \"SELECT feature_id FROM EpigenomicFeature WHERE name = %s AND feature_class = %s\",\n",
    "            (feature_name, feature_class)\n",
    "        )\n",
    "        res = cursor.fetchone()\n",
    "        if res:\n",
    "            feature_ids_cache_global[feature_name] = res[0]\n",
    "        else:\n",
    "            cursor.execute(\n",
    "                \"INSERT INTO EpigenomicFeature (feature_class, name) VALUES (%s, %s)\",\n",
    "                (feature_class, feature_name)\n",
    "            )\n",
    "            db.commit()\n",
    "            feature_ids_cache_global[feature_name] = cursor.lastrowid\n",
    "\n",
    "    feature_value_batch = []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"Inserting rows for {file}\", leave=False):\n",
    "        chrom = row['chrom']\n",
    "        start = int(row['start'])\n",
    "        end = int(row['end'])\n",
    "        strand = row['strand']\n",
    "        tpm = sanitize_value(row['TPM'])\n",
    "\n",
    "        region_key = (chrom, start, end, strand)\n",
    "        if region_key in region_cache:\n",
    "            region_id = region_cache[region_key]\n",
    "        else:\n",
    "            cursor.execute(\n",
    "                \"SELECT region_id FROM GenomicRegion WHERE chrom = %s AND start = %s AND end = %s AND strand = %s\",\n",
    "                region_key\n",
    "            )\n",
    "            res = cursor.fetchone()\n",
    "            if res:\n",
    "                region_id = res[0]\n",
    "            else:\n",
    "                cursor.execute(\n",
    "                    \"INSERT INTO GenomicRegion (chrom, start, end, strand) VALUES (%s, %s, %s, %s)\",\n",
    "                    region_key\n",
    "                )\n",
    "                db.commit()\n",
    "                region_id = cursor.lastrowid\n",
    "            region_cache[region_key] = region_id\n",
    "\n",
    "        activity_key = (region_id, cell_line_id, region_type_id, window_size_id)\n",
    "        if activity_key in activity_cache:\n",
    "            activity_id = activity_cache[activity_key]\n",
    "        else:\n",
    "            cursor.execute(\n",
    "                \"SELECT activity_id FROM EpigenomicActivity WHERE region_id = %s AND cell_line_id = %s AND region_type_id = %s AND window_size_id = %s\",\n",
    "                activity_key\n",
    "            )\n",
    "            res = cursor.fetchone()\n",
    "            if res:\n",
    "                activity_id = res[0]\n",
    "            else:\n",
    "                cursor.execute(\n",
    "                    \"INSERT INTO EpigenomicActivity (region_id, cell_line_id, region_type_id, window_size_id, TPM) VALUES (%s, %s, %s, %s, %s)\",\n",
    "                    (*activity_key, tpm)\n",
    "                )\n",
    "                db.commit()\n",
    "                activity_id = cursor.lastrowid\n",
    "            activity_cache[activity_key] = activity_id\n",
    "\n",
    "        for feature_name in feature_cols:\n",
    "            fold_change = sanitize_value(row[feature_name])\n",
    "            if fold_change is None:\n",
    "                continue\n",
    "            feature_id = feature_ids_cache_global[feature_name]\n",
    "            feature_value_batch.append((activity_id, feature_id, fold_change))\n",
    "\n",
    "        if len(feature_value_batch) >= 100000:\n",
    "            cursor.executemany(\n",
    "                \"INSERT INTO FeatureValue (activity_id, feature_id, fold_change) VALUES (%s, %s, %s)\",\n",
    "                feature_value_batch\n",
    "            )\n",
    "            db.commit()\n",
    "            feature_value_batch = []\n",
    "\n",
    "    if feature_value_batch:\n",
    "        cursor.executemany(\n",
    "            \"INSERT INTO FeatureValue (activity_id, feature_id, fold_change) VALUES (%s, %s, %s)\",\n",
    "            feature_value_batch\n",
    "        )\n",
    "        db.commit()\n",
    "\n",
    "    with open(processed_files_path, \"a\") as f:\n",
    "        f.write(file + \"\\n\")\n",
    "\n",
    "    print(f\"✅ Finished processing: {file}\")\n",
    "\n",
    "cursor.close()\n",
    "db.close()\n",
    "print(\"✅ All files processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948c4fa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
