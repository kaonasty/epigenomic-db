{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e17c64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting CSV-based FFNN Training\n",
      "ğŸ” Scanning CSVs for all features...\n",
      "ğŸ“ˆ Total unified features: 947\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 12.5 GiB for an array with shape (1772960, 947) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 88\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸš€ Starting CSV-based FFNN Training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     87\u001b[0m start_data_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 88\u001b[0m X, y, used_features \u001b[38;5;241m=\u001b[39m \u001b[43mload_all_data_from_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCELL_LINES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mREGION_TYPES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTPM_VALUES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m X_train, X_temp, y_train, y_temp \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, stratify\u001b[38;5;241m=\u001b[39my, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     90\u001b[0m X_val, X_test, y_val, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X_temp, y_temp, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, stratify\u001b[38;5;241m=\u001b[39my_temp, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "Cell \u001b[1;32mIn[13], line 80\u001b[0m, in \u001b[0;36mload_all_data_from_csv\u001b[1;34m(cell_lines, region_types, tpm_values)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m X:\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo matching data found for selected filters.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 80\u001b[0m X_all \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m y_all \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(y)\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X_all, y_all, all_feature_names\n",
      "File \u001b[1;32mc:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\_core\\shape_base.py:291\u001b[0m, in \u001b[0;36mvstack\u001b[1;34m(tup, dtype, casting)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    290\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m (arrs,)\n\u001b[1;32m--> 291\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 12.5 GiB for an array with shape (1772960, 947) and data type float64"
     ]
    }
   ],
   "source": [
    "# ğŸ“¦ Required Libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "import gc\n",
    "import joblib\n",
    "\n",
    "# ğŸ“ Path to All CSVs\n",
    "BASE_PATH = r\"I:\\Final2\\Max Data\"\n",
    "all_csv_files = glob(os.path.join(BASE_PATH, \"*.csv\"))\n",
    "\n",
    "# âœ… Settings\n",
    "CELL_LINES = [\"A549\", \"GM12878\", \"H1\", \"HEK293\", \"HepG2\", \"K562\", \"MCF-7\"]\n",
    "REGION_TASKS = {\n",
    "    \"IE vs IP\": ([\"enhancer\", \"promoter\"], [0.0]),\n",
    "    \"AP vs IP\": ([\"promoter\"], [1.0, 0.0]),\n",
    "    \"AE vs IE\": ([\"enhancer\"], [1.0, 0.0]),\n",
    "    \"AE vs AP\": ([\"enhancer\", \"promoter\"], [1.0])\n",
    "}\n",
    "SELECTED_TASK = \"AP vs IP\"\n",
    "REGION_TYPES, TPM_VALUES = REGION_TASKS[SELECTED_TASK]\n",
    "\n",
    "# --- Feature Columns ---\n",
    "FIXED_COLUMNS = [\"chrom\", \"start\", \"end\", \"strand\", \"TPM\"]\n",
    "\n",
    "# --- Data Loader Function ---\n",
    "def load_all_data_from_csv(cell_lines, region_types, tpm_values):\n",
    "    print(\"ğŸ” Scanning CSVs for all features...\")\n",
    "    all_feature_names = set()\n",
    "\n",
    "    # 1. Collect all feature names\n",
    "    for file_path in all_csv_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        parts = filename.split(\"_\")\n",
    "        if len(parts) < 3:\n",
    "            continue\n",
    "        cell_line, _, region_type = parts[0], parts[1], parts[2].replace(\"_processed.csv\", \"\")\n",
    "        if cell_line not in cell_lines or region_type not in region_types:\n",
    "            continue\n",
    "        df = pd.read_csv(file_path, nrows=1)  # only read header\n",
    "        feature_cols = [col for col in df.columns if col not in FIXED_COLUMNS]\n",
    "        all_feature_names.update(feature_cols)\n",
    "\n",
    "    all_feature_names = sorted(all_feature_names)\n",
    "    print(f\"ğŸ“ˆ Total unified features: {len(all_feature_names)}\")\n",
    "\n",
    "    # 2. Load all data and align to same features\n",
    "    X, y = [], []\n",
    "    for file_path in all_csv_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        parts = filename.split(\"_\")\n",
    "        if len(parts) < 3:\n",
    "            continue\n",
    "        cell_line, _, region_type = parts[0], parts[1], parts[2].replace(\"_processed.csv\", \"\")\n",
    "\n",
    "        if cell_line not in cell_lines or region_type not in region_types:\n",
    "            continue\n",
    "\n",
    "        df = pd.read_csv(file_path)\n",
    "        df = df[df[\"TPM\"].isin(tpm_values)]\n",
    "        if df.empty:\n",
    "            continue\n",
    "\n",
    "        feature_df = df.drop(columns=FIXED_COLUMNS, errors=\"ignore\")\n",
    "        aligned_features = feature_df.reindex(columns=all_feature_names, fill_value=0).values\n",
    "        X.append(aligned_features)\n",
    "        y.append((df[\"TPM\"] > 0).astype(int).values)\n",
    "\n",
    "    if not X:\n",
    "        raise ValueError(\"No matching data found for selected filters.\")\n",
    "\n",
    "    X_all = np.vstack(X)\n",
    "    y_all = np.concatenate(y)\n",
    "    return X_all, y_all, all_feature_names\n",
    "\n",
    "# --- Main Training ---\n",
    "print(\"ğŸš€ Starting CSV-based FFNN Training\")\n",
    "\n",
    "start_data_time = time.time()\n",
    "X, y, used_features = load_all_data_from_csv(CELL_LINES, REGION_TYPES, TPM_VALUES)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "data_time = time.time() - start_data_time\n",
    "\n",
    "print(f\"ğŸ“Š Data loaded: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "print(f\"ğŸ•’ Data load time: {data_time:.2f} sec\")\n",
    "\n",
    "start_train_time = time.time()\n",
    "model = build_model(X.shape[1])\n",
    "model.compile(optimizer=Adam(0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=10, verbose=1, validation_data=(X_val, y_val))\n",
    "train_time = time.time() - start_train_time\n",
    "\n",
    "# --- Evaluation ---\n",
    "val_pred = (model.predict(X_val) > 0.5).astype(int)\n",
    "val_acc = accuracy_score(y_val, val_pred)\n",
    "\n",
    "test_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "test_acc = accuracy_score(y_test, test_pred)\n",
    "\n",
    "print(f\"âœ… Validation Accuracy: {val_acc:.4f}\")\n",
    "print(f\"ğŸ” Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"ğŸ§  Training time: {train_time:.2f} sec\")\n",
    "\n",
    "# --- Save Model\n",
    "model_path = \"trained_ffnn_model_csv.pkl\"\n",
    "joblib.dump(model, model_path)\n",
    "print(f\"ğŸ’¾ Model saved to: {model_path}\")\n",
    "\n",
    "# --- Cleanup\n",
    "K.clear_session()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1c12cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting Batch-based FFNN Training\n",
      "ğŸ“Š Total matched rows: 1,772,960\n",
      "ğŸ“¦ Batching into 10 batches of ~177,296 rows\n",
      "ğŸ§¬ Total unified features: 947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Loading batch 1/10 ...\n",
      "ğŸ§  Training on 177296 samples\n",
      "Epoch 1/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.8648 - loss: 0.4140\n",
      "Epoch 2/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8644 - loss: 0.3971\n",
      "Epoch 3/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8652 - loss: 0.3957\n",
      "Epoch 4/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8644 - loss: 0.3971\n",
      "Epoch 5/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8650 - loss: 0.3960\n",
      "Epoch 6/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8634 - loss: 0.3989\n",
      "Epoch 7/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8640 - loss: 0.3979\n",
      "Epoch 8/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8661 - loss: 0.3940\n",
      "Epoch 9/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8640 - loss: 0.3977\n",
      "Epoch 10/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8644 - loss: 0.3971\n",
      "\n",
      "ğŸ” Loading batch 2/10 ...\n",
      "ğŸ§  Training on 177296 samples\n",
      "Epoch 1/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8607 - loss: 0.4040\n",
      "Epoch 2/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8634 - loss: 0.3989\n",
      "Epoch 3/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.8607 - loss: 0.4039\n",
      "Epoch 4/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8624 - loss: 0.4008\n",
      "Epoch 5/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - accuracy: 0.8623 - loss: 0.4010\n",
      "Epoch 6/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8626 - loss: 0.4004\n",
      "Epoch 7/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8631 - loss: 0.3995\n",
      "Epoch 8/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8622 - loss: 0.4011\n",
      "Epoch 9/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8631 - loss: 0.3996\n",
      "Epoch 10/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8628 - loss: 0.4001\n",
      "\n",
      "ğŸ” Loading batch 3/10 ...\n",
      "ğŸ§  Training on 177296 samples\n",
      "Epoch 1/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - accuracy: 0.8672 - loss: 0.3919\n",
      "Epoch 2/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - accuracy: 0.8677 - loss: 0.3908\n",
      "Epoch 3/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8671 - loss: 0.3921\n",
      "Epoch 4/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8669 - loss: 0.3925\n",
      "Epoch 5/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8686 - loss: 0.3893\n",
      "Epoch 6/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8670 - loss: 0.3924\n",
      "Epoch 7/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8692 - loss: 0.3881\n",
      "Epoch 8/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.8668 - loss: 0.3927\n",
      "Epoch 9/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8663 - loss: 0.3936\n",
      "Epoch 10/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8672 - loss: 0.3918\n",
      "\n",
      "ğŸ” Loading batch 4/10 ...\n",
      "ğŸ§  Training on 177296 samples\n",
      "Epoch 1/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8711 - loss: 0.3845\n",
      "Epoch 2/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8708 - loss: 0.3849\n",
      "Epoch 3/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8710 - loss: 0.3847\n",
      "Epoch 4/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8706 - loss: 0.3855\n",
      "Epoch 5/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8708 - loss: 0.3851\n",
      "Epoch 6/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8702 - loss: 0.3862\n",
      "Epoch 7/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8709 - loss: 0.3848\n",
      "Epoch 8/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - accuracy: 0.8712 - loss: 0.3844\n",
      "Epoch 9/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8718 - loss: 0.3830\n",
      "Epoch 10/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8699 - loss: 0.3867\n",
      "\n",
      "ğŸ” Loading batch 5/10 ...\n",
      "ğŸ§  Training on 177296 samples\n",
      "Epoch 1/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.8685 - loss: 0.3893\n",
      "Epoch 2/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8680 - loss: 0.3902\n",
      "Epoch 3/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.8687 - loss: 0.3889\n",
      "Epoch 4/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8684 - loss: 0.3896\n",
      "Epoch 5/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.8692 - loss: 0.3881\n",
      "Epoch 6/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8669 - loss: 0.3922\n",
      "Epoch 7/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8691 - loss: 0.3882\n",
      "Epoch 8/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8682 - loss: 0.3899\n",
      "Epoch 9/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8684 - loss: 0.3896\n",
      "Epoch 10/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.8690 - loss: 0.3885\n",
      "\n",
      "ğŸ” Loading batch 6/10 ...\n",
      "ğŸ§  Training on 177296 samples\n",
      "Epoch 1/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8943 - loss: 0.3376\n",
      "Epoch 2/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8928 - loss: 0.3408\n",
      "Epoch 3/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.8926 - loss: 0.3412\n",
      "Epoch 4/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.8926 - loss: 0.3412\n",
      "Epoch 5/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8928 - loss: 0.3408\n",
      "Epoch 6/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8930 - loss: 0.3402\n",
      "Epoch 7/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8923 - loss: 0.3417\n",
      "Epoch 8/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8931 - loss: 0.3401\n",
      "Epoch 9/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8941 - loss: 0.3380\n",
      "Epoch 10/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - accuracy: 0.8930 - loss: 0.3403\n",
      "\n",
      "ğŸ” Loading batch 7/10 ...\n",
      "ğŸ§  Training on 177296 samples\n",
      "Epoch 1/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8559 - loss: 0.4127\n",
      "Epoch 2/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8570 - loss: 0.4104\n",
      "Epoch 3/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8557 - loss: 0.4129\n",
      "Epoch 4/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8564 - loss: 0.4115\n",
      "Epoch 5/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8586 - loss: 0.4076\n",
      "Epoch 6/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8563 - loss: 0.4117\n",
      "Epoch 7/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8566 - loss: 0.4113\n",
      "Epoch 8/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8575 - loss: 0.4097\n",
      "Epoch 9/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8565 - loss: 0.4113\n",
      "Epoch 10/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8565 - loss: 0.4114\n",
      "\n",
      "ğŸ” Loading batch 8/10 ...\n",
      "ğŸ§  Training on 177296 samples\n",
      "Epoch 1/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8544 - loss: 0.4151\n",
      "Epoch 2/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8534 - loss: 0.4169\n",
      "Epoch 3/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8537 - loss: 0.4164\n",
      "Epoch 4/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8549 - loss: 0.4143\n",
      "Epoch 5/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8536 - loss: 0.4165\n",
      "Epoch 6/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8547 - loss: 0.4146\n",
      "Epoch 7/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8545 - loss: 0.4149\n",
      "Epoch 8/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8537 - loss: 0.4163\n",
      "Epoch 9/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.8552 - loss: 0.4138\n",
      "Epoch 10/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8540 - loss: 0.4158\n",
      "\n",
      "ğŸ” Loading batch 9/10 ...\n",
      "ğŸ§  Training on 177296 samples\n",
      "Epoch 1/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8703 - loss: 0.3860\n",
      "Epoch 2/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8698 - loss: 0.3869\n",
      "Epoch 3/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8685 - loss: 0.3894\n",
      "Epoch 4/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8704 - loss: 0.3856\n",
      "Epoch 5/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8706 - loss: 0.3853\n",
      "Epoch 6/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - accuracy: 0.8715 - loss: 0.3836\n",
      "Epoch 7/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.8706 - loss: 0.3853\n",
      "Epoch 8/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.8715 - loss: 0.3837\n",
      "Epoch 9/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - accuracy: 0.8708 - loss: 0.3849\n",
      "Epoch 10/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.8718 - loss: 0.3830\n",
      "\n",
      "ğŸ” Loading batch 10/10 ...\n",
      "ğŸ§  Training on 177296 samples\n",
      "Epoch 1/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - accuracy: 0.8643 - loss: 0.3972\n",
      "Epoch 2/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - accuracy: 0.8649 - loss: 0.3960\n",
      "Epoch 3/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.8643 - loss: 0.3971\n",
      "Epoch 4/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.8652 - loss: 0.3954\n",
      "Epoch 5/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.8633 - loss: 0.3990\n",
      "Epoch 6/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.8635 - loss: 0.3986\n",
      "Epoch 7/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.8635 - loss: 0.3987\n",
      "Epoch 8/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.8640 - loss: 0.3978\n",
      "Epoch 9/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.8629 - loss: 0.3998\n",
      "Epoch 10/10\n",
      "\u001b[1m5541/5541\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.8641 - loss: 0.3976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ Model saved to: trained_ffnn_model_csv_batches.h5\n",
      "âœ… Total training time: 1358.34 sec\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“¦ Required Libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "import gc\n",
    "import joblib\n",
    "\n",
    "# ğŸ“ Path to All CSVs\n",
    "BASE_PATH = r\"I:\\Final2\\Max Data\"\n",
    "all_csv_files = glob(os.path.join(BASE_PATH, \"*.csv\"))\n",
    "\n",
    "# âœ… Settings\n",
    "CELL_LINES = [\"A549\", \"GM12878\", \"H1\", \"HEK293\", \"HepG2\", \"K562\", \"MCF-7\"]\n",
    "REGION_TASKS = {\n",
    "    \"IE vs IP\": ([\"enhancer\", \"promoter\"], [0.0]),\n",
    "    \"AP vs IP\": ([\"promoter\"], [1.0, 0.0]),\n",
    "    \"AE vs IE\": ([\"enhancer\"], [1.0, 0.0]),\n",
    "    \"AE vs AP\": ([\"enhancer\", \"promoter\"], [1.0])\n",
    "}\n",
    "SELECTED_TASK = \"AP vs IP\"\n",
    "REGION_TYPES, TPM_VALUES = REGION_TASKS[SELECTED_TASK]\n",
    "FIXED_COLUMNS = [\"chrom\", \"start\", \"end\", \"strand\", \"TPM\"]\n",
    "NUM_BATCHES = 10\n",
    "\n",
    "# ğŸ”¢ Count total rows and store file info\n",
    "def count_total_rows(cell_lines, region_types, tpm_values):\n",
    "    total = 0\n",
    "    file_info = []\n",
    "\n",
    "    for file_path in all_csv_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        parts = filename.split(\"_\")\n",
    "        if len(parts) < 3:\n",
    "            continue\n",
    "        cell_line, _, region_type = parts[0], parts[1], parts[2].replace(\"_processed.csv\", \"\")\n",
    "        if cell_line in cell_lines and region_type in region_types:\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, usecols=[\"TPM\"])\n",
    "                count = df[df[\"TPM\"].isin(tpm_values)].shape[0]\n",
    "                total += count\n",
    "                file_info.append((file_path, count))\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Error reading {file_path}: {e}\")\n",
    "    return total, file_info\n",
    "\n",
    "# ğŸ§¬ Get unified feature list\n",
    "def get_unified_feature_names(cell_lines, region_types):\n",
    "    feature_names = set()\n",
    "    for file_path in all_csv_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        parts = filename.split(\"_\")\n",
    "        if len(parts) < 3:\n",
    "            continue\n",
    "        cell_line, _, region_type = parts[0], parts[1], parts[2].replace(\"_processed.csv\", \"\")\n",
    "        if cell_line in cell_lines and region_type in region_types:\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, nrows=1)\n",
    "                feature_names.update([col for col in df.columns if col not in FIXED_COLUMNS])\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Skipping {file_path} due to error: {e}\")\n",
    "    return sorted(feature_names)\n",
    "\n",
    "# ğŸ“¥ Load next batch of rows\n",
    "def load_next_batch(file_info, feature_list, tpm_values, batch_size, offset):\n",
    "    X_batch, y_batch = [], []\n",
    "    rows_loaded = 0\n",
    "\n",
    "    for file_path, row_count in file_info:\n",
    "        if rows_loaded >= batch_size:\n",
    "            break\n",
    "\n",
    "        df = pd.read_csv(file_path)\n",
    "        df = df[df[\"TPM\"].isin(tpm_values)]\n",
    "\n",
    "        if offset > 0:\n",
    "            if offset >= len(df):\n",
    "                offset -= len(df)\n",
    "                continue\n",
    "            df = df.iloc[offset:]\n",
    "            offset = 0\n",
    "\n",
    "        to_take = min(batch_size - rows_loaded, len(df))\n",
    "        df = df.iloc[:to_take]\n",
    "        aligned = df.drop(columns=FIXED_COLUMNS, errors=\"ignore\").reindex(columns=feature_list, fill_value=0).values\n",
    "        labels = (df[\"TPM\"] > 0).astype(int).values\n",
    "\n",
    "        X_batch.append(aligned)\n",
    "        y_batch.append(labels)\n",
    "        rows_loaded += len(df)\n",
    "\n",
    "    if not X_batch:\n",
    "        return None, None\n",
    "\n",
    "    return np.vstack(X_batch), np.concatenate(y_batch)\n",
    "\n",
    "# ğŸ§  Build a simple FFNN model\n",
    "def build_model(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "# ğŸ MAIN PROCESS\n",
    "print(\"ğŸš€ Starting Batch-based FFNN Training\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Step 1: Count rows and get info\n",
    "total_rows, file_info = count_total_rows(CELL_LINES, REGION_TYPES, TPM_VALUES)\n",
    "rows_per_batch = total_rows // NUM_BATCHES\n",
    "print(f\"ğŸ“Š Total matched rows: {total_rows:,}\")\n",
    "print(f\"ğŸ“¦ Batching into {NUM_BATCHES} batches of ~{rows_per_batch:,} rows\")\n",
    "\n",
    "# Step 2: Get feature list\n",
    "ALL_FEATURES = get_unified_feature_names(CELL_LINES, REGION_TYPES)\n",
    "print(f\"ğŸ§¬ Total unified features: {len(ALL_FEATURES)}\")\n",
    "\n",
    "# Step 3: Initialize and compile model\n",
    "model = build_model(len(ALL_FEATURES))\n",
    "model.compile(optimizer=Adam(0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 4: Train in batches\n",
    "offset = 0\n",
    "for batch in range(NUM_BATCHES):\n",
    "    print(f\"\\nğŸ” Loading batch {batch+1}/{NUM_BATCHES} ...\")\n",
    "    X_batch, y_batch = load_next_batch(file_info, ALL_FEATURES, TPM_VALUES, rows_per_batch, offset)\n",
    "    offset += rows_per_batch\n",
    "\n",
    "    if X_batch is None:\n",
    "        print(\"âš ï¸ No data for batch, skipping...\")\n",
    "        continue\n",
    "\n",
    "    print(f\"ğŸ§  Training on {X_batch.shape[0]} samples\")\n",
    "    model.fit(X_batch, y_batch, epochs=10, verbose=1)\n",
    "\n",
    "# ğŸ§ª Optional: evaluate on a final held-out portion\n",
    "# (This can be improved by separating out a small validation set in `load_next_batch`)\n",
    "# For now, you could save the model:\n",
    "\n",
    "model_path = \"trained_ffnn_model_csv_batches.h5\"\n",
    "model.save(model_path)\n",
    "print(f\"\\nğŸ’¾ Model saved to: {model_path}\")\n",
    "\n",
    "# ğŸ§¹ Cleanup\n",
    "K.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "print(f\"âœ… Total training time: {time.time() - start_time:.2f} sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3035d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting FFNN Training (CSV Version)\n",
      "ğŸ“Š Total matched rows: 1,772,960\n",
      "ğŸ§¬ Total unified features: 947\n",
      "ğŸ“¥ Data loaded in 222.70 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“¦ Required Libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "import gc\n",
    "import joblib\n",
    "\n",
    "# ğŸ“ Path to All CSVs\n",
    "BASE_PATH = r\"I:\\Final2\\Max Data\"\n",
    "all_csv_files = glob(os.path.join(BASE_PATH, \"*.csv\"))\n",
    "\n",
    "# âœ… Settings\n",
    "CELL_LINES = [\"A549\", \"GM12878\", \"H1\", \"HEK293\", \"HepG2\", \"K562\", \"MCF-7\"]\n",
    "REGION_TASKS = {\n",
    "    \"IE vs IP\": ([\"enhancer\", \"promoter\"], [0.0]),\n",
    "    \"AP vs IP\": ([\"promoter\"], [1.0, 0.0]),\n",
    "    \"AE vs IE\": ([\"enhancer\"], [1.0, 0.0]),\n",
    "    \"AE vs AP\": ([\"enhancer\", \"promoter\"], [1.0])\n",
    "}\n",
    "SELECTED_TASK = \"AP vs IP\"\n",
    "REGION_TYPES, TPM_VALUES = REGION_TASKS[SELECTED_TASK]\n",
    "FIXED_COLUMNS = [\"chrom\", \"start\", \"end\", \"strand\", \"TPM\"]\n",
    "\n",
    "# ğŸ”¢ Count total rows and store file info\n",
    "def count_total_rows(cell_lines, region_types, tpm_values):\n",
    "    total = 0\n",
    "    file_info = []\n",
    "    for file_path in all_csv_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        parts = filename.split(\"_\")\n",
    "        if len(parts) < 3:\n",
    "            continue\n",
    "        cell_line, _, region_type = parts[0], parts[1], parts[2].replace(\"_processed.csv\", \"\")\n",
    "        if cell_line in cell_lines and region_type in region_types:\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, usecols=[\"TPM\"])\n",
    "                count = df[df[\"TPM\"].isin(tpm_values)].shape[0]\n",
    "                total += count\n",
    "                file_info.append((file_path, count))\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Error reading {file_path}: {e}\")\n",
    "    return total, file_info\n",
    "\n",
    "# ğŸ§¬ Get unified feature list\n",
    "def get_unified_feature_names(cell_lines, region_types):\n",
    "    feature_names = set()\n",
    "    for file_path in all_csv_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        parts = filename.split(\"_\")\n",
    "        if len(parts) < 3:\n",
    "            continue\n",
    "        cell_line, _, region_type = parts[0], parts[1], parts[2].replace(\"_processed.csv\", \"\")\n",
    "        if cell_line in cell_lines and region_type in region_types:\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, nrows=1)\n",
    "                feature_names.update([col for col in df.columns if col not in FIXED_COLUMNS])\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Skipping {file_path} due to error: {e}\")\n",
    "    return sorted(feature_names)\n",
    "\n",
    "# ğŸ“¥ Load all filtered data\n",
    "def load_all_data(file_info, feature_list, tpm_values):\n",
    "    X_all, y_all = [], []\n",
    "    for file_path, _ in file_info:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            df = df[df[\"TPM\"].isin(tpm_values)]\n",
    "            X = df.drop(columns=FIXED_COLUMNS, errors=\"ignore\").reindex(columns=feature_list, fill_value=0).values\n",
    "            y = (df[\"TPM\"] > 0).astype(int).values\n",
    "            X_all.append(X)\n",
    "            y_all.append(y)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error loading {file_path}: {e}\")\n",
    "    return np.vstack(X_all), np.concatenate(y_all)\n",
    "\n",
    "# ğŸ§  Build a simple FFNN model\n",
    "def build_model(input_dim):\n",
    "    model = Sequential([\n",
    "        Dense(128, input_dim=input_dim, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# ğŸ MAIN PROCESS\n",
    "print(\"ğŸš€ Starting FFNN Training (CSV Version)\")\n",
    "\n",
    "start_data_time = time.time()\n",
    "\n",
    "# Step 1: Count rows and get info\n",
    "total_rows, file_info = count_total_rows(CELL_LINES, REGION_TYPES, TPM_VALUES)\n",
    "print(f\"ğŸ“Š Total matched rows: {total_rows:,}\")\n",
    "\n",
    "# Step 2: Get feature list\n",
    "ALL_FEATURES = get_unified_feature_names(CELL_LINES, REGION_TYPES)\n",
    "print(f\"ğŸ§¬ Total unified features: {len(ALL_FEATURES)}\")\n",
    "\n",
    "# Step 3: Load all filtered data\n",
    "X, y = load_all_data(file_info, ALL_FEATURES, TPM_VALUES)\n",
    "data_load_time = time.time() - start_data_time\n",
    "print(f\"ğŸ“¥ Data loaded in {data_load_time:.2f} seconds\")\n",
    "\n",
    "# Step 4: Split data\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "# Step 5: Train the model\n",
    "start_train_time = time.time()\n",
    "model = build_model(X.shape[1])\n",
    "model.compile(optimizer=Adam(0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=512, verbose=1)\n",
    "train_time = time.time() - start_train_time\n",
    "print(f\"ğŸ§  Training completed in {train_time:.2f} seconds\")\n",
    "\n",
    "# Step 6: Evaluate\n",
    "val_pred = (model.predict(X_val) > 0.5).astype(int)\n",
    "val_acc = accuracy_score(y_val, val_pred)\n",
    "print(f\"âœ… Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "test_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "test_acc = accuracy_score(y_test, test_pred)\n",
    "print(f\"ğŸ” Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Save model\n",
    "model_path = \"trained_ffnn_model_csv_full.h5\"\n",
    "model.save(model_path)\n",
    "print(f\"ğŸ’¾ Model saved to: {model_path}\")\n",
    "\n",
    "# Cleanup\n",
    "K.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "print(f\"ğŸ All done! Total time: {time.time() - start_data_time:.2f} sec\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
